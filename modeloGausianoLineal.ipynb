{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYx0dHA59JOfamU9h3fX4L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanlu29/juanlu29/blob/gp_aprendizaje/modeloGausianoLineal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIM-iMXTpIfv",
        "colab_type": "text"
      },
      "source": [
        "Librerias importadas y constantes usadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an7GdNIpZ9Pr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modulos y constantes\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy\n",
        "import scipy.linalg\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "pi = np.pi\n",
        "\n",
        "# Los siguientes parametros son cuadrados de las desviaciones estandar\n",
        "sigma_signal_sq = 0.5 # desviación estándar del ruido usado para enmascarar la señal de entrenamiento\n",
        "sigma_b_sq = 70.5 # En distribucion gaussiana a priori de modelo de regresióón linear, desviación estándar de la ordenada\n",
        "sigma_m_sq = 1.0 # En distribucion gaussiana a priori de modelo de regresióón linear, desviación estándar de la pendiente\n",
        "\n",
        "ordenada_real = 10.\n",
        "pendiente_real = 5."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHAQxULhqQkb",
        "colab_type": "text"
      },
      "source": [
        "Informacion sobre la particion del sistema y las realizaciones a realizar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho6aiRGIqLSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_r = 4 # numero de realizaciones del proceso gaussiano\n",
        "\n",
        "n_tp = 500 # puntos para muestrear el fondo continuo para la desviacion estándar\n",
        "\n",
        "x_bot_b = -2.5\n",
        "x_top_b = 14.\n",
        "\n",
        "x = np.linspace(x_bot_b,x_top_b,n_tp)\n",
        "\n",
        "n_test = 40 # puntos para realizar inferencia\n",
        "\n",
        "x_train = np.linspace(x_bot_b,x_top_b,n_test)\n",
        "y_train = np.fromiter([ordenada_real+pendiente_real*x+random.gauss(0,np.sqrt(sigma_signal_sq)) for x in x_train],float)\n",
        "#y_train = np.fromiter([60*np.sin(x/3.)+random.gauss(0,np.sqrt(sigma_signal_sq)) for x in x_train],float)\n",
        "\n",
        "#x = np.linspace(-30.,80.,n_tp)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoa5hD_3pRet",
        "colab_type": "text"
      },
      "source": [
        "La siguiente celda contiene informacion sobre el conjunto de puntos - objetivos sobre el que entrenar el proceso gausiano, así como una especificación a priori de la matriz de covarianzas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNrT3V5mpQ2T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "45a65765-b30f-46d2-dfd4-dda219df6a24"
      },
      "source": [
        "\n",
        "# Hacemos la proyeccion sobre el espacio propio del modelo de regresion linear y derivamos la matriz de dispersion de los parámetros\n",
        "proyeccionlinear = espacioPropio()\n",
        "aleatorio = espacioPropio()\n",
        "#mCov_x,phi_x,mean = proyeccionlinear.formaBilinealAleatoria(x,sigma_m_sq)\n",
        "#mCov_train,phi_train,mean_train = proyeccionlinear.formaBilinealAleatoria(x_train,sigma_m_sq)\n",
        "\n",
        "# Modelo lineal\n",
        "#K_prior,K_train_prior,K_train_train,phi_x,phi_t,mean = proyeccionlinear.linear(x,x_train,sigma_b_sq,sigma_m_sq)\n",
        "# Kernel aleatorio\n",
        "K_prior,K_train_prior,K_train_train,phi_x,phi_t,mean = aleatorio.covarianzaAleatoria(x,x_train,sigma_m_sq)\n",
        "# Generamos un proceso gausiano para generar la distribucion de funciones con probabilidades a priori (las que nos ofrece las incertidumbres en los parametros)\n",
        "gp_cov_diag = gaussProcess(K_prior,K_train_prior,K_train_train, np.zeros(n_tp),phi_x)\n",
        "sigma_prior = gp_cov_diag.sigmaCalc(gp_cov_diag.K_prior)\n",
        "\n",
        "# Entrenamos con los datos proyectados sobre el espacio propio de esta regresion\n",
        "gp_cov_diag.entrenarGP(phi_t,y_train,sigma_signal_sq)\n",
        "sigma_pred = gp_cov_diag.sigmaCalc(gp_cov_diag.K_pred)\n",
        "\n",
        "\n",
        "\n",
        "visualizacion = datosVisualizacion()\n",
        "\n",
        "# Predictivo\n",
        "\n",
        "visualizacion.add_sigma(x,sigma_pred)\n",
        "visualizacion.add_mean(x,gp_cov_diag.mean_pred)\n",
        "#visualizacion.add_mean(x,np.zeros(len(x)))\n",
        "visualizacion.add_train_data(x_train,y_train)\n",
        "\n",
        "[visualizacion.add_data(x,gp_cov_diag.distribucionGP(gp_cov_diag.mean_pred,gp_cov_diag.L_pred)[0],inset) for inset in range(n_r) ]\n",
        "[visualizacion.add_data_plot(inset) for inset in range(n_r) ]\n",
        "\n",
        "# Prior\n",
        "\n",
        "#visualizacion.add_sigma(x,sigmaVal)\n",
        "#visualizacion.add_mean(x,np.zeros(n_tp))\n",
        "#visualizacion.add_train_data(x_train,y_train)\n",
        "\n",
        "#[visualizacion.add_data(x,gp_cov_diag.distribucionGP(gp_cov_diag.mean_prior,gp_cov_diag.L_prior)[0],inset) for inset in range(n_r) ]\n",
        "#[visualizacion.add_data_plot(inset) for inset in range(n_r) ]\n",
        "\n",
        "visualizacion.mostrarPlot()\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LinAlgError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-500e26c97de0>\u001b[0m in \u001b[0;36mcholDescomp\u001b[0;34m(self, K)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m       \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/linalg/decomp_cholesky.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m     90\u001b[0m     c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n\u001b[0;32m---> 91\u001b[0;31m                          check_finite=check_finite)\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/linalg/decomp_cholesky.py\u001b[0m in \u001b[0;36m_cholesky\u001b[0;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[1;32m     39\u001b[0m         raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n\u001b[0;32m---> 40\u001b[0;31m                           \"definite\" % info)\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLinAlgError\u001b[0m: 2-th leading minor of the array is not positive definite",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-52bf7e7c494e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mK_prior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK_train_prior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK_train_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphi_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphi_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maleatorio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovarianzaAleatoria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma_m_sq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Generamos un proceso gausiano para generar la distribucion de funciones con probabilidades a priori (las que nos ofrece las incertidumbres en los parametros)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgp_cov_diag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgaussProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_prior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK_train_prior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK_train_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphi_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msigma_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp_cov_diag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmaCalc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgp_cov_diag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-500e26c97de0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, K_p, K_p_t, K_t_t, Mean, Input)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK_train_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK_t_t\u001b[0m \u001b[0;31m# Es la matriz de covarianzas entre los datos de entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholDescomp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcholDescomp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-500e26c97de0>\u001b[0m in \u001b[0;36mcholDescomp\u001b[0;34m(self, K)\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/linalg/decomp_cholesky.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \"\"\"\n\u001b[1;32m     90\u001b[0m     c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n\u001b[0;32m---> 91\u001b[0;31m                          check_finite=check_finite)\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/linalg/decomp_cholesky.py\u001b[0m in \u001b[0;36m_cholesky\u001b[0;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n\u001b[0;32m---> 40\u001b[0;31m                           \"definite\" % info)\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         raise ValueError('LAPACK reported an illegal value in {}-th argument'\n",
            "\u001b[0;31mLinAlgError\u001b[0m: 2-th leading minor of the array is not positive definite"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uCgm8NJqE0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REjNF1UyXdut",
        "colab_type": "text"
      },
      "source": [
        "Este google colab sera un espacio donde practicar conceptos básicos sobre procesos gausianos, en general desarrollar clases para reproducir los conceptos básicos de estos y practicar alguna regresión \"de juguete\". Muestrea funciones del espacio F:R¹\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dIxZad-Xbvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class espacioPropio():\n",
        "  '''\n",
        "  Esta clase define el espacio propio donde proyectamos y la matriz de covarianza resultante\n",
        "  '''\n",
        "\n",
        "  def linear(self,x,x_train, sigma_b,sigma_m):\n",
        "    '''\n",
        "    Se interpretan sigma_b y sigma_m como varianzas de distribuciones a priori gaussianas\n",
        "    para la ordenada en origen y para la pendiente de nuestro modelo linear.\n",
        "    x es el input\n",
        "    '''\n",
        "    phi_x = np.array([np.array([1,xe]) for xe in x])\n",
        "    phi_t = np.array([np.array([1,xe]) for xe in x_train])\n",
        "    eps = np.array([[sigma_b,0],[0,sigma_m]])\n",
        "    K_prior = np.matmul(np.matmul(phi_x,eps),phi_x.T)\n",
        "    K_pred_train = np.matmul(np.matmul(phi_x,eps),phi_t.T) # Es la matriz de covarianzas entre datos de la distribución a priori y el entrenamiento\n",
        "    K_train_train = np.matmul(np.matmul(phi_t,eps),phi_t.T) # Es la matriz de covarianzas entre los datos de entrenamiento\n",
        "\n",
        "    return K_prior,K_pred_train,K_train_train, phi_x, phi_t, np.zeros(x.size)\n",
        "\n",
        "  def covarianzaProceso(self,phi_x,covM):\n",
        "    '''\n",
        "    Devuelve la matriz de covarianza correspondiente a distribucion de probabilidad a priori en base a la incertidumbre de los parametros de una regresion lineal .\n",
        "    '''\n",
        "    return np.matmul(np.matmul(phi_x,covM),phi_x.T)\n",
        "\n",
        "  def covarianzaAleatoria(self,x, x_train,sigma):\n",
        "    '''\n",
        "    Genera matriz de covarianza con correlaciones aleatorias sobre el espacio en que realizamos el proceso gaussiano,\n",
        "    generamos una matriz de la aplicacion aleatoria dada una distribucion normal Sigma ~ normal(0,1)\n",
        "    '''\n",
        "\n",
        "    phi_x = x\n",
        "    phi_t = x_train\n",
        "    K_prior = np.array([[random.gauss(0,1.) for xi in x ] for xj in x])\n",
        "    K_pred_train = np.array([[random.gauss(0,1.) for xi in x ] for xj in x_train]) # Es la matriz de covarianzas entre datos de la distribución a priori y el entrenamiento\n",
        "    K_train_train = np.array([[random.gauss(0,1.) for xi in x_train ] for xj in x_train]) # Es la matriz de covarianzas entre los datos de entrenamiento\n",
        "\n",
        "    return K_prior,K_pred_train,K_train_train, phi_x, phi_t, np.zeros(x.size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class gaussProcess():\n",
        "  def __init__(self,K_p,K_p_t,K_t_t,Mean, Input):\n",
        "    '''\n",
        "    Los objetos de esta clase modelan procesos gaussianos caracterizados por su promedio y covarianza\n",
        "    Eps es la matriz de covarianzas de los pesos del proceso gausiano\n",
        "    '''\n",
        "    self.x = Input # Dominio de la regresion\n",
        "    self.K_prior = K_p # Matriz de covarianzas usada para definir la distribucion a priori y mas tarde para muestrear funciones del proceso gausiano\n",
        "    self.K_pred_train = K_p_t # Es la matriz de covarianzas entre datos de la distribución a priori y el entrenamiento\n",
        "    self.K_train_train = K_t_t # Es la matriz de covarianzas entre los datos de entrenamiento\n",
        "    self.mean_prior = Mean\n",
        "    self.L_prior = self.cholDescomp(self.K_prior)\n",
        "\n",
        "  def cholDescomp(self,K):\n",
        "    '''\n",
        "    Cholesky decomposition\n",
        "    '''\n",
        "    try:\n",
        "      L = scipy.linalg.cholesky(K, lower=True)\n",
        "    except:\n",
        "      L = scipy.linalg.cholesky(K + np.diag(0.01*np.ones(int(np.sqrt(K.size)))), lower=True)\n",
        "\n",
        "    return L\n",
        "\n",
        "\n",
        "  def entrenarGP(self,entrenamiento_x,entrenamiento_y,noiseLevel):\n",
        "    '''\n",
        "    Dados unos datos de entrenamiento, recupera el proceso gaussiano con la distribucion a posteriori condicionando el GP a las observaciones\n",
        "    noiseLevel es la desviación estándar cuadrática del ruido sobre los puntos del conjunto de datos de entrenamiento\n",
        "    '''\n",
        "    self.xtrain = entrenamiento_x\n",
        "    self.ytrain = entrenamiento_y\n",
        "\n",
        "    # Definimos nueva covarianza\n",
        "    # Calculo de matriz inversa de la covarianza. \n",
        "    try:\n",
        "      self.invK = np.linalg.inv(self.K_train_train + noiseLevel*np.diag(np.ones(len(self.xtrain))))\n",
        "    except:\n",
        "      print(\"La matriz inversa no existe porque el determinante es cero\")\n",
        "      \n",
        "    self.K_pred = self.K_prior - np.matmul(np.matmul(self.K_pred_train,self.invK),self.K_pred_train.T)\n",
        "    \n",
        "    self.L_pred = self.cholDescomp(self.K_pred)\n",
        "\n",
        "    self.mean_pred = np.matmul(np.matmul(self.K_pred_train,self.invK),self.ytrain)\n",
        "\n",
        "\n",
        "  def distribucionGP(self,mean,L):\n",
        "    '''\n",
        "    Genera realizaciones del proceso gaussiano dada la descomposicion cholesky de la matriz de covarianzas y la media correspondiente\n",
        "    '''\n",
        "    gaussNumbers = np.fromiter([ random.gauss(0,1) for x in range(len(np.diag(L))) ],float)\n",
        "    return [mean + np.matmul(L,gaussNumbers.T)]\n",
        "\n",
        "  def sigmaCalc(self,cov):\n",
        "    '''\n",
        "    Desviacion estándar punto a punto del proceso gausiano dado por la matriz de covarianza \n",
        "    Es la raiz cuadrado de los elementos de la diagonal de la matriz de covarianzas del proceso generado\n",
        "    '''\n",
        "    return np.sqrt(np.diag(cov))\n",
        "\n",
        "class datosVisualizacion():\n",
        "  '''\n",
        "  Esta clase contiene los datos de entrenamiento y realizaciones de los procesos gaussianos para la visualización de los mismos\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "    self.sigma = []\n",
        "    self.realizacion_dict = {}\n",
        "    self.inicializacionPlot()\n",
        "\n",
        "  def add_mean(self,X,Mean):\n",
        "    '''\n",
        "    Almacena la desviación estándar de un proceso gausiano dado\n",
        "    '''\n",
        "    self.x_mean = X\n",
        "    self.mean = Mean\n",
        "\n",
        "\n",
        "  def add_sigma(self,X,Sigma):\n",
        "    '''\n",
        "    Almacena la desviación estándar de un proceso gausiano dado\n",
        "    '''\n",
        "    self.x_sigma = X\n",
        "    self.sigma = Sigma\n",
        "    self.meanMarker = 'r-'\n",
        "\n",
        "  def add_train_data(self,X,Y):\n",
        "    self.xtrain = X\n",
        "    self.ytrain = Y\n",
        "    self.marker = 'b*'\n",
        "\n",
        "  def add_data(self,X,Y,label):\n",
        "    self.x.append(X)\n",
        "    self.y.append(Y)\n",
        "    self.realizacion_dict[label] = len(self.x)-1\n",
        "\n",
        "  def inicializacionPlot(self):\n",
        "    '''\n",
        "    Inicializa el objeto matplotlib\n",
        "    '''\n",
        "    self.fig,self.ax = plt.subplots(nrows=1,ncols=1,figsize=(10,10))\n",
        "\n",
        "  def add_data_plot(self,label):\n",
        "    try:\n",
        "      self.ax.plot(self.x[self.realizacion_dict[label]],self.y[self.realizacion_dict[label]],'g--')\n",
        "    except:\n",
        "      print('No existe datos referentes a etiqueta :',label)\n",
        "\n",
        "  def add_train_data_plot(self):\n",
        "    try:\n",
        "      self.ax.plot(self.xtrain,self.ytrain,self.marker)\n",
        "    except:\n",
        "      print('No existen datos de entrenamiento')\n",
        "\n",
        "  def add_mean_plot(self):\n",
        "    try:\n",
        "      self.ax.plot(self.x_mean,self.mean,self.meanMarker)\n",
        "    except:\n",
        "      print('No existen datos de entrenamiento')\n",
        "\n",
        "\n",
        "\n",
        "  def contorno_sigma(self):\n",
        "    '''\n",
        "    Muestra en pantalla contorno de 1 y 2 veces los valores de sigma del proceso gausiano\n",
        "    '''\n",
        "    self.ax.fill(np.concatenate((self.x_sigma,self.x_sigma[::-1])),\n",
        "             np.concatenate((self.mean - 1.9600 * self.sigma,\n",
        "                             (self.mean + 1.9600 * self.sigma)[::-1])),\n",
        "             alpha=.45, fc='y', ec='None', label='95% confidence interval')\n",
        "    self.ax.fill(np.concatenate((self.x_sigma,self.x_sigma[::-1])),\n",
        "             np.concatenate((self.mean - 1.000 * self.sigma,\n",
        "                             (self.mean + 1.000 * self.sigma)[::-1])),\n",
        "             alpha=.35, fc='b', ec='None', label='68% confidence interval')\n",
        "\n",
        "\n",
        "  def mostrarPlot(self):\n",
        "    '''\n",
        "    Muestra plot\n",
        "    '''\n",
        "    self.add_mean_plot()\n",
        "    self.add_train_data_plot()\n",
        "    self.contorno_sigma()\n",
        "    self.ax.grid(True)\n",
        "    self.fig.show()\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMf7KCEkil6k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a80a42d5-2070-48be-b071-283630774a77"
      },
      "source": [
        "A = np.array([[1,3],[4,2]])\n",
        "B = np.linalg.inv(A)\n",
        "print(np.matmul(B,A))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.00000000e+00 -1.11022302e-16]\n",
            " [ 0.00000000e+00  1.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}