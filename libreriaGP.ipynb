{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGi0YvuvRmwjt2aVDCpzG/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanlu29/juanlu29/blob/gp_aprendizaje/libreriaGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juguGpOpG3HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8e3df8cd-6d55-452e-b4a4-db137e71a837"
      },
      "source": [
        "# Modulos y constantes\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy\n",
        "import scipy.linalg\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW6CbMb8KZmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pruebas con funciones anonimas en python\n",
        "\n",
        "class funcionesPrueba():\n",
        "  def __init__(self):\n",
        "    '''\n",
        "    Covarianza puede ser una cadena de texto o una funcion anónima como combinación de metodos de la clase\n",
        "    covarianza es un diccionario\n",
        "    '''\n",
        "    self.Kernels = self.kernels()\n",
        "\n",
        "  def calcCovM(self,x,y):\n",
        "    '''\n",
        "    Dados x e y como ndarrays, calcula la matriz de covarianzas cuyos elementos son evaluados de la forma\n",
        "    C(xi,yi) = (k(xi,yi)) con rango (nx x ny) donde nx y ny son la longitud de los vectores x  e y respectivamente.\n",
        "    hiper son los hiperparámetros usados en el modelo\n",
        "    '''\n",
        "    # Se crean matrices [[x],[x],...] (longitud de la concatenacion nx) y [[y],[y],...].T (longitud de la concatenacion ny)\n",
        "    xM = np.full((len(y),len(x)),x)\n",
        "    yM = (np.full((len(x),len(y)),y)).T\n",
        "    # Llamamos la funcion Ker que nos debe devolver como salida una matriz cuyos elementos calculados representan k(x,y)\n",
        "    return self.Ker(xM,yM,*self.Hiper)\n",
        "\n",
        "\n",
        "  def compCov(self,covarianza,*hiper):\n",
        "    '''\n",
        "    Permite definir el kernel a usar por el proceso gaussiano, tanto si es uno de los kernels definidos en la clase como si es una combinacion de los mismos\n",
        "    El argumento es una funcion anónima en caso de ser una composición, en caso contrario es suficiente con proporcionar el nombre del kernel definido\n",
        "    hiper son los hiperparámetros que precisan el kernel y vienen dado por un diccionario\n",
        "    '''\n",
        "    self.Hiper = hiper\n",
        "    self.Ker = lambda x_test, x_obs, *hiper: covarianza(x_test,x_obs,*self.Hiper) # Covarianza definida\n",
        "\n",
        "  class kernels():\n",
        "    '''\n",
        "    Se definen kernels de uso común junto a sus hiperparámetros\n",
        "    '''\n",
        "    def __init__(self):\n",
        "      return\n",
        "\n",
        "    def exponencialCuadrada(self,xb,xa,s,l):\n",
        "      '''\n",
        "      Correlacion estacionaria (depende solo de la distancia r = abs(xb-xa)) con decaimiento exponencial cuadratico\n",
        "      Hiperparámetros:\n",
        "      s : amplitud de la correlacion, esta relacionado con la amplitud de las trayectorias generadas por el GP\n",
        "      l : escala de longitud del GP. Esta relacionada con la distancia sobr la cual los procesos generados por el GP pueden presentar oscilaciones o variaciones\n",
        "      '''\n",
        "      return np.power(s,2)*np.exp(-np.power(xa-xb,2)/(2.*np.power(l,2)))\n",
        "\n",
        "    def ruidoBlanco(self,xb,xa,sigma):\n",
        "      '''\n",
        "      Correlaciones de ruido blanco o gaussiano puro, solo correlaciona los puntos consigo mismo. \n",
        "      Representa incertidumbres intrínsecas en las observaciones y asume una distribución a priori de esos errores gaussiano\n",
        "      Hiperparáámetros:\n",
        "      s : amplitud o incertidumbre del ruido generado\n",
        "      '''\n",
        "      return np.where(xb==xa, np.power(sigma,2),0.) # Se usa el metodo numpy.where porque las operaciones logicas sobre arrays son consideradas ambiguas en el intérprete de python\n",
        "                                                    # Esta manera es válidad para python para comprar elemento a elemento entre matrices."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN9rSp4AQ88-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "439e37a9-f355-4eb2-a86c-5347fc5a1563"
      },
      "source": [
        "x_test = np.linspace(1.,5.,4)\n",
        "x_obs = np.linspace(1.,5.,4)\n",
        "x_obs = x_obs.T\n",
        "print(x_test,x_obs)\n",
        "prueba = funcionesPrueba()\n",
        "s = 3.\n",
        "l = 4.\n",
        "sigma = 0.\n",
        "prueba.compCov(lambda x,y,s,l,sigma: prueba.Kernels.exponencialCuadrada(x,y,s,l)+prueba.Kernels.ruidoBlanco(x,y,sigma),s,l,sigma) # Aqui especificamos los hiperparametros\n",
        "print(prueba.calcCovM(x_test,x_obs,s,l,sigma))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.         2.33333333 3.66666667 5.        ] [1.         2.33333333 3.66666667 5.        ]\n",
            "[[9.         8.51363522 7.20663663 5.45877594]\n",
            " [8.51363522 9.         8.51363522 7.20663663]\n",
            " [7.20663663 8.51363522 9.         8.51363522]\n",
            " [5.45877594 7.20663663 8.51363522 9.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i290_E_ZUG9H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2d38376c-2107-4ed5-eee6-ed2866184bc3"
      },
      "source": [
        "print(x_test,x_obs)\n",
        "x = 3.66666667\n",
        "y = 5.\n",
        "a = 3.*3.*np.exp(-np.power((x-y),2)/(2.*4.*4.))\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.         2.33333333 3.66666667 5.        ] [1.         2.33333333 3.66666667 5.        ]\n",
            "8.513635222525789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djR_jiyCGlbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class gaussProcess():\n",
        "  def __init__(self):\n",
        "    '''\n",
        "    Los objetos de esta clase modelan procesos gaussianos caracterizados por su promedio y covarianza\n",
        "    '''\n",
        "    #self.Derivadas = prior_Partial_Der\n",
        "    self.Kernels = self.kernels()\n",
        "\n",
        "  def calcCovM(self,x,y,*hiper):\n",
        "    '''\n",
        "    Dados x e y como ndarrays, calcula la matriz de covarianzas cuyos elementos son evaluados de la forma\n",
        "    C(xi,yi) = (k(xi,yi)) con rango (nx x ny) donde nx y ny son la longitud de los vectores x  e y respectivamente.\n",
        "    hiper son los hiperparámetros usados en el modelo\n",
        "    '''\n",
        "    # Se crean matrices [[x],[x],...] (longitud de la concatenacion nx) y [[y],[y],...].T (longitud de la concatenacion ny)\n",
        "    xM = np.full((len(y),len(x)),x)\n",
        "    yM = (np.full((len(x),len(y)),y)).T\n",
        "    # Llamamos la funcion Ker que nos debe devolver como salida una matriz cuyos elementos calculados representan k(x,y)\n",
        "    return self.Ker(xM,yM,*hiper)\n",
        "\n",
        "  def calcDerCovM(self,x,y):\n",
        "    '''\n",
        "    Dados x e y como ndarrays, calcula la matriz correspondiente a la derivada respecto los hiperparámetros de las covarianzas\n",
        "    cuyos elementos son evaluados de la forma C_j(xi,yi) = (dk(xi,yi)dh_j) con rango (nx x ny) donde nx y ny son la longitud de los \n",
        "    vectores x e y respectivamente. Hiper son los hiperparámetros usados en el modelo\n",
        "    '''\n",
        "    # Se crean matrices [[x],[x],...] (longitud de la concatenacion nx) y [[y],[y],...].T (longitud de la concatenacion ny)\n",
        "    xM = np.full((len(y),len(x)),x)\n",
        "    yM = (np.full((len(x),len(y)),y)).T\n",
        "    # Llamamos la funcion Ker que nos debe devolver como salida una matriz cuyos elementos calculados representan k(x,y)\n",
        "    #return [ self.derKer[i](xM,yM,*hiper) for i in range(self.N_iter) ] \n",
        "    for ff, dd in self.todo:\n",
        "      print(dd)\n",
        "    return np.asarray([ func(xM,yM,*hipero) for func, hipero in self.todo ] )\n",
        "\n",
        "  def compCov(self,covarianza,*hiper):\n",
        "    '''\n",
        "    Permite definir el kernel a usar por el proceso gaussiano, tanto si es uno de los kernels definidos en la clase como si es una combinacion de los mismos\n",
        "    El argumento es una funcion anónima en caso de ser una composición, en caso contrario es suficiente con proporcionar el nombre del kernel definido\n",
        "    hiper son los hiperparámetros que precisan el kernel y vienen dado por un diccionario\n",
        "    '''\n",
        "    self.Hiper = hiper\n",
        "    self.N_iter = len(hiper)\n",
        "    self.Ker = lambda x_1, x_2, *hiper: covarianza(x_1,x_2,*hiper) # Covarianza definida\n",
        "\n",
        "  def compDerCov(self,derCovarianza,*hiper):\n",
        "    '''\n",
        "    Permite definir la expresion para la derivada del kernel respecto los distintos hiperparametros como un iterable zip\n",
        "    de funciones anonimas juntos a sus hiperparámetros.\n",
        "    En este caso por propositos de optimizacion se espera que se evalue dinamicamente con distintos hiperparámetros\n",
        "    '''\n",
        "    self.todo = zip(derCovarianza,hiper)\n",
        "    try:\n",
        "      self.derKer = [ lambda x_1,x_2,*hipero: derCovarianza[i](x_1,x_2,*hipero) for func,hipero in self.todo ] \n",
        "    except:\n",
        "      raise NameError(\"No se ha definido kernel previamente\")\n",
        "    return\n",
        "\n",
        "\n",
        "  class kernels():\n",
        "    '''\n",
        "    Se definen kernels de uso común junto a sus hiperparámetros y derivadas asociadas\n",
        "    '''\n",
        "    def __init__(self):\n",
        "      return\n",
        "\n",
        "    def exponencialCuadrada(self,xb,xa,s,l):\n",
        "      '''\n",
        "      Correlacion estacionaria (depende solo de la distancia r = abs(xb-xa)) con decaimiento exponencial cuadratico\n",
        "\n",
        "      k(xb,xa) = s² exp( - (xb-xa)²/(2 l²) )\n",
        "\n",
        "      Hiperparámetros:\n",
        "      s : amplitud de la correlacion, esta relacionado con la amplitud de las trayectorias generadas por el GP\n",
        "      l : escala de longitud del GP. Esta relacionada con la distancia sobr la cual los procesos generados por el GP pueden presentar oscilaciones o variaciones\n",
        "      '''\n",
        "      return np.power(s,2)*np.exp(-np.power(xa-xb,2)/(2.*np.power(l,2)))\n",
        "\n",
        "    def expCua_der_s(self,xb,xa,s,l):\n",
        "      '''\n",
        "      Derivada respecto el hiperparámetro s de las correlaciones estacionarias con decaimiento exponenciales cuadráticas\n",
        "\n",
        "      k(xb,xa) = 2s exp( - (xb-xa)²/(2 l²) )\n",
        "\n",
        "      Hiperparámetros:\n",
        "      s : amplitud de la correlacion, esta relacionado con la amplitud de las trayectorias generadas por el GP\n",
        "      l : escala de longitud del GP. Esta relacionada con la distancia sobr la cual los procesos generados por el GP pueden presentar oscilaciones o variaciones\n",
        "      '''\n",
        "      return s*np.exp(-np.power(xa-xb,2)/(2.*np.power(l,2)))\n",
        "\n",
        "    def expCua_der_l(self,xb,xa,s,l):\n",
        "      '''\n",
        "      Derivada respecto el hiperparámetro l de las correlaciones estacionarias con decaimiento exponenciales cuadráticas\n",
        "\n",
        "      k(xb,xa) = s²(xb-xa)² / l³ exp( - (xb-xa)²/(2 l²) )\n",
        "\n",
        "      Hiperparámetros:\n",
        "      s : amplitud de la correlacion, esta relacionado con la amplitud de las trayectorias generadas por el GP\n",
        "      l : escala de longitud del GP. Esta relacionada con la distancia sobr la cual los procesos generados por el GP pueden presentar oscilaciones o variaciones\n",
        "      '''\n",
        "      return (np.power(s*(xa-xb),2)/np.power(l,3))*np.exp(-np.power(xa-xb,2)/(2.*np.power(l,2)))\n",
        "\n",
        "    def ruidoBlanco(self,xb,xa,sigma):\n",
        "      '''\n",
        "      Correlaciones de ruido blanco o gaussiano puro, solo correlaciona los puntos consigo mismo. \n",
        "\n",
        "      k(xb,xa) = sigma² delta(xb,xa)\n",
        "\n",
        "      Representa incertidumbres intrínsecas en las observaciones y asume una distribución a priori de esos errores gaussiano\n",
        "      Hiperparáámetros:\n",
        "      s : amplitud o incertidumbre del ruido generado\n",
        "      '''\n",
        "      return np.where(xb==xa, np.power(sigma,2),0.) # Se usa el metodo numpy.where porque las operaciones logicas sobre arrays son consideradas ambiguas en el intérprete de python\n",
        "                                                    # Esta manera es válidad para python para comprar elemento a elemento entre matrices.\n",
        "\n",
        "    def rB_der_sigma(self,xb,xa,sigma):\n",
        "      '''\n",
        "      Derivada respecto el hiperparámetro sigma del ruido no correlacionado\n",
        "\n",
        "      k(xb,xa) = sigma² delta(xb,xa)\n",
        "\n",
        "      Representa incertidumbres intrínsecas en las observaciones y asume una distribución a priori de esos errores gaussiano\n",
        "      Hiperparáámetros:\n",
        "      s : amplitud o incertidumbre del ruido generado\n",
        "      '''\n",
        "      return np.where(xb==xa, 2*sigma,0.) # Se usa el metodo numpy.where porque las operaciones logicas sobre arrays son consideradas ambiguas en el intérprete de python\n",
        "                                          # Esta manera es válidad para python para comprar elemento a elemento entre matrices.\n",
        "\n",
        " \n",
        "  def cholDescomp(self,K):\n",
        "    '''\n",
        "    Descomposicion cholesky de una matriz dada\n",
        "    '''\n",
        "    try:\n",
        "      L = scipy.linalg.cholesky(K, lower=True)\n",
        "    except:\n",
        "     # L = scipy.linalg.cholesky(K + np.diag(0.000001*np.ones(int(np.sqrt(K.size)))), lower=True)\n",
        "      raise NameError(\"No es posible efectuar descomposicion cholesky\")\n",
        "\n",
        "    return L\n",
        "\n",
        "  def calcInvK(self,matriz):\n",
        "    '''\n",
        "    Calculo de la inversa de matriz. \n",
        "    '''\n",
        "    # Calculo de matriz inversa de la covarianza. \n",
        "    try:\n",
        "      invK = np.linalg.inv(matriz)\n",
        "    except:\n",
        "      raise NameError(\"No es posible calcular matriz inversa\")\n",
        "\n",
        "    return invK\n",
        "\n",
        "  def condicionarGP(self,dominio,observaciones,*dominio_test):\n",
        "    '''\n",
        "    Dadas unas observaciones junto a sus valores de dominio (uni-dimensionales) en el rango sobre el que se modela el GP, \n",
        "    obtiene la covarianza asociada a la distribución de trayectorias condicionadas al conjunto de partida dado un dominio \n",
        "    de test en el que queremos comprobar la predicción. Por tanto permite generar las trayectorias compatibles con los datos\n",
        "\n",
        "    También calcula el logaritmo de la probabilidad de verosimilitud marginal (marginal likelihood) de las observaciones dados los inputs \n",
        "    y parámetros del modelo usado.\n",
        "\n",
        "    Este algoritmo esta especificado en el libro \"Gaussian Processes for Machine Learning\", como algoritmo 2.1\n",
        "    '''\n",
        "    self.xtrain = dominio\n",
        "    self.ytrain = observaciones\n",
        "    self.xtest = dominio_test\n",
        "\n",
        "    # Definimos nueva covarianza\n",
        "    # Primero se realiza la descomposición cholesky de la adición de la covarianza de la distribución a priori,\n",
        "    # para lo cual calculamos la matriz de covarianza asociada al dominio de las observaciones sobre si mismo\n",
        "    # y al cruce del dominio de las observaciones y del test de prediccion.\n",
        "    try:\n",
        "      self.Cov_obs_obs = self.calcCovM(self.xtrain,self.xtrain,*self.Hiper)\n",
        "      self.Cov_obs_test = self.calcCovM(self.xtrain,self.xtest,*self.Hiper)\n",
        "    except:\n",
        "      raise NameError(\"Debes especificar primero el kernel que estas usando junto a sus hiperparametros con el metodo self.compCov\")\n",
        "\n",
        "    cholL = np.linalg.cholesky(self.Cov_obs_obs)\n",
        "\n",
        "    # Calculo del vector alfa como solucion del sistema K_obs_obs*alfa = y_entrenamiento usando la descomposición cholesky anterior\n",
        "    self.Alfa = scipy.linalg.cho_solve((cholL,True),self.ytrain)\n",
        "\n",
        "    # Vector v solución particular del sistema cholL*v = k_test para todos los datos del entrenamiento.\n",
        "    # Este sistema se resuelve introduciendo k_star no como matriz columna sino como la submatriz calculada como la evaluación de la covarianza a priori de los inputs de los valores a predecir y los inputs del entrenamiento\n",
        "    v = scipy.linalg.solve_triangular(cholL,self.cov_obs_test, lower=True)\n",
        "\n",
        "    # Se obtiene el valor promedio del proceso predictivo condicionado a las observaciones\n",
        "    self.Media_pred = np.matmul(self.Cov_obs_test.T,self.Alfa)\n",
        "\n",
        "    # Covarianza predictiva o condicionada. El proceso llevado a cabo ha sido obtener la distribución condicionada del GP a las observaciones\n",
        "    # Intenta calcular directamente la matriz, en caso contrario calcular la matriz de covarianza de la distribucion a priori dado el test\n",
        "    try:\n",
        "      self.Cov_pred = self.Cov_priori - np.matmul(v.T,v)\n",
        "    except:\n",
        "      # En caso que no esté definida o no coincida con la dimensionalidad del test, se recalcula la covarianza a priori\n",
        "      self.Cov_priori = self.calcCovM(self.xtest,self.xtest,*self.Hiper)\n",
        "      self.Cov_pred = self.Cov_priori - np.matmul(v.T,v)\n",
        "\n",
        "    # La siguiente matriz descompuesta es necesaria para generar procesos del GP.\n",
        "    self.L_pred = self.cholDescomp(self.Cov_pred)\n",
        "  \n",
        "\n",
        "  def log_prob_verosimilitud_datos(self,dominio,observaciones):\n",
        "    '''\n",
        "    Dadas unas observaciones, kernel e hiperparámetros dados, calcula el logaritmo de la probabilidad de verosimilitud de las observaciones al modelo y sus parámetros\n",
        "    '''\n",
        "\n",
        "    # Primero se realiza la descomposición cholesky de la adición de la covarianza de la distribución a priorir y el término de ruido.\n",
        "    cholL = np.linalg.cholesky(self.calcCovM(dominio,dominio,*self.Hiper))\n",
        "\n",
        "    # Calculo del vector alfa como solucion del sistema K*alfa = y_entrenamiento usando la descomposición cholesky anterior\n",
        "    alfa = scipy.linalg.cho_solve((cholL,True),observaciones)\n",
        "\n",
        "    # Calculo de la probabilidad de verosimilitud marginal a este modelo y sus parámetros\n",
        "    self.log_marg_y = - 0.5*np.dot(alfa,observaciones) - np.sum(np.log(np.diag(cholL)),axis=0) - (float(len(dominio))/2.)*np.log(2*np.pi)\n",
        "    if (self.log_marg_y < -70.):\n",
        "      self.log_marg_y = 0.\n",
        "\n",
        "  def prioriGP(self,x_test):\n",
        "    '''\n",
        "    Computa los términos necesarios para extraer realizaciones del GP siguiendo la distribución a priori\n",
        "    '''\n",
        "    try:\n",
        "      self.Cov_priori = self.calcCovM(self.xtest,self.xtest,*self.Hiper)\n",
        "    except:\n",
        "      raise NameError(\"Debes especificar primero el kernel que estas usando junto a sus hiperparametros con el metodo self.compCov\")\n",
        "\n",
        "    self.xtest = x_test\n",
        "\n",
        "    # La siguiente matriz descompuesta es necesaria para generar procesos del GP.\n",
        "    self.L_priori = self.cholDescomp(self.Cov_pred)\n",
        "\n",
        "\n",
        "  def procesosPrioriGP(self):\n",
        "    '''\n",
        "    Genera realizaciones del proceso gaussiano dada la descomposicion cholesky de la matriz de covarianzas y la media correspondiente\n",
        "    '''\n",
        "    # Si no se ha calculado la descomposicióón de la covarianza se llama al método aqui\n",
        "    try:\n",
        "      return self.Mean_priori +  np.matmul(self.L_priori,(np.randn(len(self.xtest))).T)\n",
        "    except:\n",
        "      raise NameError(\"No se ha incluido los valores del dominio sobre los que muestrear el proceso o no se ha calculado la descomposición de la covarianza\")\n",
        "\n",
        "  def procesosCondicionadosGP(self):\n",
        "    '''\n",
        "    Genera realizaciones del proceso gaussiano dada la descomposicion cholesky de la matriz de covarianzas y la media correspondiente\n",
        "    '''\n",
        "    try:\n",
        "      return self.Media_pred + np.matmul(self.L_pred,(np.randn(len(self.xtest))).T)\n",
        "    except:\n",
        "      raise NameError(\"El GP no ha sido condicionado a las observaciones\")\n",
        "\n",
        "  def sigmaCalc(self,cov):\n",
        "    '''\n",
        "    Desviacion estándar punto a punto del proceso gausiano dado por la matriz de covarianza \n",
        "    Es la raiz cuadrado de los elementos de la diagonal de la matriz de covarianzas del proceso generado\n",
        "    '''\n",
        "    return np.sqrt(np.diag(cov))\n",
        "\n",
        "\n",
        "  def correlacionPuntoPunto(self):\n",
        "    '''\n",
        "    Obtiene la correlacion punto a punto dado un inputo concreto por el indice i_x\n",
        "    '''\n",
        "    try:\n",
        "      self.corrPP_pred = np.array([ self.K_pred[i,:] for i in range(len(self.K_pred)) ])\n",
        "    except:\n",
        "      print(\" Proceso gaussiano no entrenado aún, no existe K_pred \")\n",
        "    self.corrPP_prior = np.array([ self.K_prior[i,:] for i in range(len(self.K_prior)) ])\n",
        "\n",
        "  def derivadaVerosimilitudMarginal(self,*args):\n",
        "    '''\n",
        "    Dadas unas observaciones particulares estima la derivada del logaritmo de la probabilidad asociada a la distribución de verosimilitud marginal asociada al modelo respecto sus hiperparámetros\n",
        "    '''\n",
        "\n",
        "    if not isinstance(args[0], np.ndarray):\n",
        "      raise NameError(\"Las observaciones dadas para calcular la derivada del logaritmo de la distribución marginal de verosimilitud no es un array con datos\")\n",
        "    else:\n",
        "      observaciones = args[0]\n",
        "      observaciones = observaciones.reshape(len(observaciones),1) # Esta operacion es necesaria para que mas tarde el producto matmul nos reproduzca una matriz en lugar de producto escalar entre vectores.\n",
        "\n",
        "    # Vector auxiliar alpha\n",
        "    try:\n",
        "      alpha = np.matmul(self.invK,observaciones)\n",
        "    except:\n",
        "      raise NameError(\"Necesitas calcular la matriz inversa de covarianzas de la distribución a priori\")\n",
        "\n",
        "    alpha_alpha = np.matmul(alpha,alpha.T) \n",
        "    alpha_alpha_minus_invK = alpha_alpha - self.invK\n",
        "\n",
        "    # Calculo. Es el valor de la derivada del logaritmo\n",
        "    var_incSignal = self.Ruido*np.sum(alpha_alpha_minus_invK)# Si asumimos como hiperparámetro la amplitud del ruido sobre las medidas respecto la señal verdadera, se debe añadir este término\n",
        "    self.derVerMarg = [ 0.5*np.trace(np.matmul(alpha_alpha_minus_invK,self.Derivadas[i])) for i in range(len(self.Derivadas))]\n",
        "    self.derVerMarg.append(var_incSignal)\n",
        "    self.derVerMarg = np.asarray(self.derVerMarg)\n",
        "  \n",
        "  def derivada_LO_CV(self,*args):\n",
        "    '''\n",
        "    Dadas unas observaciones particulares estima la derivada de la suma de los logaritmos de la evaluación de la distribución de probabilidad marginal de verosimilitud a que subconjuntos\n",
        "    de las observaciones están condicionadas al resto, respecto a los hiperparámetros del modelo. Si se encuentra el máximo de este valor, estamos asegurándonos de que con los hiperparámetros ajustados\n",
        "    las \"predicciones\" tras generar trayectorias del proceso gausiano que corresponderían a los subconjuntos excluidos sucesivamente son las más probables que el modelo puede hacer. Al ser la suma, esta optimización \n",
        "    es global y abarca la aproximación de observacion/test de todos los subconjuntos.\n",
        "    '''\n",
        "\n",
        "    if not isinstance(args[0], np.ndarray):\n",
        "      raise NameError(\"Las observaciones dadas para calcular la derivada del logaritmo de la distribución marginal de verosimilitud no es un array con datos\")\n",
        "    else:\n",
        "      observaciones = args[0]\n",
        "\n",
        "    # Vector auxiliar alpha y matriz auxiliar Z y la diagonal de la inversa de la distribucion a priori\n",
        "    try:\n",
        "      alpha = np.matmul(self.invK,observaciones)\n",
        "      invK_diag = np.diag(self.invK)\n",
        "      zeta = [np.matmul(self.invK,self.Derivadas[i]) for i in range(len(self.Derivadas)) ] # Es una lista de matrices\n",
        "      zeta_K = [np.diag(np.matmul(zeta[i],self.invK)) for i in range(len(self.Derivadas)) ]\n",
        "      alpha_invK = np.multiply(alpha,1./invK_diag)\n",
        "    except:\n",
        "      raise NameError(\"Necesitas calcular la matriz inversa de covarianzas de la distribución a priori\")\n",
        "    \n",
        "    termino_1 = [np.matmul(np.matmul(alpha_invK,zeta[i]),alpha) for i in range(len(self.Derivadas)) ] # Debido a Z esto es una lista\n",
        "    termino_2 = [np.sum(np.multiply(np.multiply(0.5*(1. + np.multiply(np.power(alpha,2),1./invK_diag)),zeta_K[i]),1./invK_diag)) for i in range(len(self.Derivadas)) ] # Debido a zeta_K esto es una lista\n",
        "\n",
        "    # Calculo. Es el valor de la derivada del logaritmo\n",
        "    self.derVerLOO = np.asarray([ termino_1[i] + termino_2[i] for i in range(len(self.Derivadas))])\n",
        "    "
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Ogka96GyNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class datosVisualizacion():\n",
        "  '''\n",
        "  Esta clase contiene los datos de entrenamiento y realizaciones de los procesos gaussianos para la visualización de los mismos\n",
        "  '''\n",
        "  def __init__(self,Titulo):\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "    self.sigma = []\n",
        "    self.realizacion_dict = {}\n",
        "    self.titulo = Titulo\n",
        "    self.inicializacionPlot()\n",
        "\n",
        "  def inicializacionPlot(self):\n",
        "    '''\n",
        "    Inicializa el objeto matplotlib\n",
        "    '''\n",
        "    self.fig,self.ax = plt.subplots(nrows=1,ncols=1,figsize=(10,10))\n",
        "    self.ax.set_title(self.titulo)\n",
        "    self.fig_hm, self.ax_hm = plt.subplots(nrows=1,ncols=1,figsize=(10,10))\n",
        "\n",
        "  def add_covariance(self, cov):\n",
        "    '''\n",
        "    Añada matriz de covarianza y alternativamente calcula la covarianza del proceso gaussiano sobre el dominio\n",
        "    '''\n",
        "\n",
        "    self.cov_GP = cov\n",
        "\n",
        "  def add_mean(self,X,Mean):\n",
        "    '''\n",
        "    Almacena la desviación estándar de un proceso gausiano dado\n",
        "    '''\n",
        "    self.x_mean = X\n",
        "    self.mean = Mean\n",
        "\n",
        "\n",
        "  def add_sigma(self,X,Sigma):\n",
        "    '''\n",
        "    Almacena la desviación estándar de un proceso gausiano dado\n",
        "    '''\n",
        "    self.x_sigma = X\n",
        "    self.sigma = Sigma\n",
        "    self.meanMarker = 'r-'\n",
        "\n",
        "  def add_train_data(self,X,Y):\n",
        "    self.xtrain = X\n",
        "    self.ytrain = Y\n",
        "    self.marker = 'b*'\n",
        "\n",
        "  def add_data(self,X,Y,label):\n",
        "    self.x.append(X)\n",
        "    self.y.append(Y)\n",
        "    self.realizacion_dict[label] = len(self.x)-1\n",
        "\n",
        "  def add_corrPuntoPunto(self,corrPP, corrInputs):\n",
        "    '''\n",
        "    Añade correlaciones punto a punto sobre un dominio dado por los valores corrInputs\n",
        "    '''\n",
        "    self.corrPP = corrPP\n",
        "    self.corr_in = corrInputs\n",
        "\n",
        "  def add_data_plot(self,label):\n",
        "    try:\n",
        "      self.ax.plot(self.x[self.realizacion_dict[label]],self.y[self.realizacion_dict[label]],'g--')\n",
        "    except:\n",
        "      print('No existe datos referentes a etiqueta :',label)\n",
        "\n",
        "  def add_train_data_plot(self):\n",
        "    try:\n",
        "      self.ax.plot(self.xtrain,self.ytrain,self.marker)\n",
        "    except:\n",
        "      print('No existen datos de entrenamiento')\n",
        "\n",
        "  def add_mean_plot(self):\n",
        "    try:\n",
        "      self.ax.plot(self.x_mean,self.mean,self.meanMarker)\n",
        "    except:\n",
        "      print('No existen datos de entrenamiento')\n",
        "\n",
        "\n",
        "\n",
        "  def contorno_sigma(self):\n",
        "    '''\n",
        "    Muestra en pantalla contorno de 1 y 2 veces los valores de sigma del proceso gausiano\n",
        "    '''\n",
        "    self.ax.fill(np.concatenate((self.x_sigma,self.x_sigma[::-1])),\n",
        "             np.concatenate((self.mean - 1.9600 * self.sigma,\n",
        "                             (self.mean + 1.9600 * self.sigma)[::-1])),\n",
        "             alpha=.45, fc='y', ec='None', label='95% confidence interval')\n",
        "    self.ax.fill(np.concatenate((self.x_sigma,self.x_sigma[::-1])),\n",
        "             np.concatenate((self.mean - 1.000 * self.sigma,\n",
        "                             (self.mean + 1.000 * self.sigma)[::-1])),\n",
        "             alpha=.35, fc='b', ec='None', label='68% confidence interval')\n",
        "    \n",
        "  def plotCorrPuntoPunto(self,indices):\n",
        "    '''\n",
        "    Visualiza correlaciones punto a punto para los inputs dados por sus indices en la lista indices como argumento del metodo\n",
        "    '''\n",
        "\n",
        "    ' Generamos y visualizamos las imagen directamente en este metodo'\n",
        "    self.fig_cpp, self.ax_cpp = plt.subplots(nrows=1,ncols=1,figsize=(10,10))\n",
        "    try:\n",
        "      [self.ax_cpp.plot(self.corr_in,self.corrPP[indice],'-') for indice in indices] \n",
        "    except:\n",
        "      print('No existe datos referentes referentes a correlaciones punto a punto')\n",
        "    self.ax_cpp.grid(True)\n",
        "    self.fig_cpp.show()\n",
        "\n",
        "\n",
        "\n",
        "  def mostrarPlot(self):\n",
        "    '''\n",
        "    Muestra plot\n",
        "    '''\n",
        "    self.add_mean_plot()\n",
        "    self.add_train_data_plot()\n",
        "    self.contorno_sigma()\n",
        "    self.ax.grid(True)\n",
        "    self.fig.show()\n",
        "\n",
        "  def hmapCov(self):\n",
        "    '''\n",
        "    Visualiza covarianza como campo bidimensional\n",
        "    '''\n",
        "    plotRange = np.arange(len(self.cov_GP))\n",
        "    heatmap = self.ax_hm.pcolormesh(self.cov_GP,cmap='RdBu_r')\n",
        "    self.ax_hm.axis([plotRange.min(), plotRange.max(), plotRange.max(), plotRange.min()])\n",
        "    self.fig_hm.colorbar(heatmap, ax=self.ax_hm)\n",
        "    self.fig_hm.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SciOihGB3HVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "bb5cd4b8-6619-4e50-9a40-6c83234867db"
      },
      "source": [
        "#x_train = np.linspace(0.1,9.7,40)\n",
        "#y_train = 1.1*np.sin(2.*x_train) + np.asarray([random.gauss(0.,1.) for i in range(len(x_train))])\n",
        "x_train = [-7.27980659640218,-6.3565864147485,-6.33349783214072,-5.85186882395103,-4.74692152231896,-4.08519569483122,-3.73024545572045,-2.78487668464404,-2.15092471829579,-0.902510956795564,0.496360463886639,0.756812013112905,1.00714394820637,2.37248404949064,2.46618962012525,4.24396641874347,4.33629137438754,4.89477951284853,5.81238911017895,6.14524950944106]\n",
        "y_train = [-1.49888963305016,0.569441996545526,0.623268356304418,0.9063943036413,-0.376423208431739,-0.871911241143502,-0.803958546300539,1.07173322993408,2.29393880644365,2.62802707180373,0.412113574676584,-0.297490218195917,-0.660456836687934,-2.66828227995347,-2.09868165955797,-0.965605061863299,-0.758001691987733,-1.32241178751454,-0.725175367478586,-0.449178680954562]\n",
        "\n",
        "\n",
        "grp = 2\n",
        "#sigmas = np.linspace(0.01,3.,grp)\n",
        "#ls = np.linspace(0.01,10.,grp)\n",
        "\n",
        "sigmas = np.linspace(np.log(0.01),np.log(3.),grp)\n",
        "ls = np.linspace(np.log(0.01),np.log(10.),grp)\n",
        "\n",
        "xx, yy = np.meshgrid(ls,sigmas)\n",
        "\n",
        "sigmas = np.exp(sigmas)\n",
        "ls = np.exp(ls)\n",
        "\n",
        "\n",
        "probabilidades = np.zeros((grp,grp))\n",
        "\n",
        "s = 1.\n",
        "\n",
        "gp_prueba = gaussProcess()\n",
        "\n",
        "\n",
        "i = 0\n",
        "for sigma in sigmas:\n",
        "\n",
        "  j = 0\n",
        "  for l in ls:\n",
        "\n",
        "    # Generamos un proceso gausiano para generar la distribucion de funciones con probabilidades a priori (las que nos ofrece las incertidumbres en los parametros)\n",
        "    #lll = np.power(10,l)\n",
        "    #sigmall = np.power(10,sigma)\n",
        "    gp_prueba.compCov(lambda x,y,s,l,sigma : gp_prueba.Kernels.exponencialCuadrada(x,y,s,l)+gp_prueba.Kernels.ruidoBlanco(x,y,sigma),s,l,sigma) # Aqui especificamos los hiperparametros\n",
        "    gp_prueba.compDerCov([lambda x,y,s,l : gp_prueba.Kernels.expCua_der_s(x,y,s,l) , lambda x,y,s,l : gp_prueba.Kernels.expCua_der_l(x,y,s,l), lambda x,y,sigma : gp_prueba.Kernels.rB_der_sigma(x,y,sigma)], [[s, l],[s, l], [sigma]] )\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "    gp_prueba.log_prob_verosimilitud_datos(x_train,y_train)\n",
        "\n",
        "\n",
        "    print(\"logaritmo de la probabilidad de verosimilitud e hiperparametros:\",gp_prueba.log_marg_y,l,sigma,i,j)\n",
        "    print(\"Matrices de las derivadas respecto los hiperparámetros :\", gp_prueba.calcDerCovM(x_train,x_train))\n",
        "  \n",
        "    j = j + 1\n",
        "  \n",
        "  i = i + 1\n",
        "\n",
        "print(\"indices al final:\",i,j,grp,np.shape(probabilidades),np.shape(xx),np.shape(yy))\n",
        "\n",
        "\n",
        "fig_3, ax_3 = plt.subplots()\n",
        "\n",
        "plotRange_sig = sigmas\n",
        "plotRange_l = ls\n",
        "\n",
        "heatmap = ax_3.pcolormesh(xx,yy,probabilidades,cmap='RdBu_r',vmin = probabilidades.min(), vmax = probabilidades.max())\n",
        "#ax_3.axis([plotRange_l.min(), plotRange_l.max(), plotRange_sig.min(), plotRange_sig.max()])\n",
        "fig_3.colorbar(heatmap, ax=ax_3)\n",
        "\n",
        "print(\"El maximo y minimo son :\", probabilidades.min(),  probabilidades.max() )\n",
        "\n",
        "#ax_3.set_xscale('log')\n",
        "#ax_3.set_yscale('log')\n",
        "\n",
        "fig_3.show()\n",
        "\n",
        "\n",
        "#im = ax_3.imshow(probabilidades)\n",
        "\n",
        "#fig_3.colorbar(im, ax=ax_3)\n",
        "\n",
        "#fig_3.show()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logaritmo de la probabilidad de verosimilitud e hiperparametros: -35.776841246090825 0.010000000000000004 0.010000000000000004 0 0\n",
            "Matrices de las derivadas respecto los hiperparámetros : []\n",
            "logaritmo de la probabilidad de verosimilitud e hiperparametros: 0.0 10.000000000000002 0.010000000000000004 0 1\n",
            "Matrices de las derivadas respecto los hiperparámetros : []\n",
            "logaritmo de la probabilidad de verosimilitud e hiperparametros: -43.146783428054256 0.010000000000000004 3.0000000000000004 1 0\n",
            "Matrices de las derivadas respecto los hiperparámetros : []\n",
            "logaritmo de la probabilidad de verosimilitud e hiperparametros: -42.85973506337433 10.000000000000002 3.0000000000000004 1 1\n",
            "Matrices de las derivadas respecto los hiperparámetros : []\n",
            "indices al final: 2 2 2 (2, 2) (2, 2) (2, 2)\n",
            "El maximo y minimo son : 0.0 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}