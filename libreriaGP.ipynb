{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7vOt07wP5jGFv9Dq5qi93",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanlu29/juanlu29/blob/gp_aprendizaje/libreriaGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juguGpOpG3HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f8650602-a5b5-4ce1-f594-e504c4c8a282"
      },
      "source": [
        "# Modulos y constantes\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy\n",
        "import scipy.linalg\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW6CbMb8KZmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pruebas con funciones anonimas en python\n",
        "\n",
        "class funcionesPrueba():\n",
        "  def __init__(self):\n",
        "    '''\n",
        "    Covarianza puede ser una cadena de texto o una funcion anónima como combinación de metodos de la clase\n",
        "    covarianza es un diccionario\n",
        "    '''\n",
        "    self.covarianzas = self.kernels()\n",
        "\n",
        "\n",
        "  def compCov(self,covarianza,**hiper):\n",
        "    '''\n",
        "    Permite definir el kernel a usar por el proceso gaussiano, tanto si es una de los kernels definidos en la clase como si es una combinacion de los mismos\n",
        "    El argumento es una funcion anónima en caso de ser una composición, en caso contrario es suficiente con proporcionar el nombre del kernel definido\n",
        "    hiper son los hiperparámetros que precisan el kernel y vienen dado por un diccionario\n",
        "    '''\n",
        "    self.Cov = lambda x_test, x_obs, **hiper: covarianza(x_test,x_obs,**hiper) # Covarianza definida\n",
        "\n",
        "  class kernels():\n",
        "    '''\n",
        "    Se definen kernels de uso común junto a sus hiperparámetros\n",
        "    '''\n",
        "    def __init__(self):\n",
        "      return\n",
        "\n",
        "    def funcion_1(self,x_test,x_obs,**hiper):\n",
        "      try:\n",
        "        h_l = hiper.get(\"l\")\n",
        "        print(h_l)\n",
        "      except:\n",
        "        raise NameError(\"No se ha especificado el hiperparametro l\")\n",
        "\n",
        "      return (x_test*x_obs)/h_l\n",
        "\n",
        "    def funcion_2(self,x_test,x_obs,**hiper):\n",
        "      try:\n",
        "        h_s = hiper.get(\"s\")\n",
        "        print(h_s)\n",
        "      except:\n",
        "        raise NameError(\"No se ha especificado el hiperparametro s\")\n",
        "\n",
        "      return h_s/(x_test*np.power(x_obs,2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN9rSp4AQ88-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "96549105-759c-402e-fa4d-dbca85aca844"
      },
      "source": [
        "x_test = np.linspace(1.,10.,5)\n",
        "x_obs = np.linspace(1.,5.,5)\n",
        "prueba = funcionesPrueba()\n",
        "prueba.compCov(lambda x,y, **hiper: prueba.covarianzas.funcion_2*prueba.covarianzas.funcion_1,l=4.,s=3.)\n",
        "print(prueba.Cov(x_test,x_obs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-48ec71e3e643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprueba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuncionesPrueba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprueba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompCov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhiper\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprueba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovarianzas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuncion_2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprueba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovarianzas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuncion_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprueba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-93-ea179b0db8c0>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x_test, x_obs, **hiper)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mhiper\u001b[0m \u001b[0mson\u001b[0m \u001b[0mlos\u001b[0m \u001b[0mhiperparámetros\u001b[0m \u001b[0mque\u001b[0m \u001b[0mprecisan\u001b[0m \u001b[0mel\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0my\u001b[0m \u001b[0mvienen\u001b[0m \u001b[0mdado\u001b[0m \u001b[0mpor\u001b[0m \u001b[0mun\u001b[0m \u001b[0mdiccionario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     '''\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhiper\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcovarianza\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_obs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mhiper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Covarianza definida\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0mkernels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-95-48ec71e3e643>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, y, **hiper)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprueba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuncionesPrueba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprueba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompCov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhiper\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprueba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovarianzas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuncion_2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprueba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovarianzas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuncion_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprueba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'method' and 'method'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djR_jiyCGlbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class espacioPropio():\n",
        "  '''\n",
        "  Esta clase define el espacio propio donde proyectamos y la matriz de covarianza resultante\n",
        "  '''\n",
        "\n",
        "  def maternCov_nu_halfIntegers(self,p,x, x_train,s,l):\n",
        "    '''\n",
        "    Expresión simplificada de la covarianza de matérn para el caso que nu sean semienteros\n",
        "\n",
        "    s es factor de escala del kernel. Permite escalar las trayectorias a la amplitud de la señal\n",
        "    '''\n",
        "\n",
        "    x_domain = np.concatenate((x_train,x), axis =None)\n",
        "\n",
        "    r  = np.asarray([ [np.abs(xi-xj) for xi in x_domain] for xj in x_domain])\n",
        "\n",
        "    ni = np.arange(p+1) # i corre desde 0 hasta el valor de p mismo\n",
        "\n",
        "    # Factor escalar de factoriales\n",
        "    gammaQuo = np.math.factorial(p)/np.math.factorial(2*p)\n",
        "\n",
        "    # El siguiente ndarray tiene dimensionalidad 3, porque estamos generando un array a partir de una lista de matrices\n",
        "    summatoryArray = np.asarray([(np.math.factorial(p + i)/(np.math.factorial(i)*np.math.factorial(p-i)))*np.power((np.sqrt(8*(p+0.5)*r)/l),p-i) for i in ni ])\n",
        "    summatory = np.sum( np.asarray(summatoryArray), axis = 0) # Axis 0 significa que solo suma corriendo el indice de la posicion cero en el array, en este caso tendriamos un array tridimensional (x,y,z) y 0 hace referencia a x\n",
        "\n",
        "    # Termino exponencial de los elementos del array\n",
        "    exponential = np.exp(-(np.sqrt(2.*(p+0.5))*r)/l)\n",
        "\n",
        "    K_total = s*gammaQuo*(np.multiply(exponential,summatory)) # Al ser un kernel estacionario, podemos interpretarlo como la evaluacion de una función dependiente de la distancia entre puntos del input\n",
        "\n",
        "    plt.imshow(K_total)\n",
        "\n",
        "    K0, K1, K2 = self.submatrices(K_total, len(x_train),len(x_train))\n",
        "\n",
        "    return K2, K1.T, K0, x, x_train, np.zeros(len(x))\n",
        "\n",
        "  def cov_drive(self,x,x_train, sigma_b,sigma_m):\n",
        "    '''\n",
        "    Obtiene la matriz de covarianza de un directorio en google drive\n",
        "    '''\n",
        "    cov_df = pd.read_csv('K_OU_experimental.csv')\n",
        "    K_total_x = cov_df.to_numpy()\n",
        "\n",
        "    K_total = K_total_x[:,1:]\n",
        "    K_total = np.array(K_total, dtype=float)\n",
        "\n",
        "    K0, K1, K2 = self.submatrices(K_total, len(x_train),len(x_train))\n",
        "\n",
        "    return K2, K1.T, K0, x, x_train, np.zeros(len(x))\n",
        "\n",
        "  def linear(self,x,x_train, sigma_b,sigma_m):\n",
        "    '''\n",
        "    Se interpretan sigma_b y sigma_m como varianzas de distribuciones a priori gaussianas\n",
        "    para la ordenada en origen y para la pendiente de nuestro modelo linear.\n",
        "    x es el input\n",
        "    '''\n",
        "    phi_x = np.array([np.array([1,xe]) for xe in x])\n",
        "    phi_t = np.array([np.array([1,xe]) for xe in x_train])\n",
        "    eps = np.array([[sigma_b,0],[0,sigma_m]])\n",
        "    K_prior = np.matmul(np.matmul(phi_x,eps),phi_x.T)\n",
        "    K_pred_train = np.matmul(np.matmul(phi_x,eps),phi_t.T) # Es la matriz de covarianzas entre datos de la distribución a priori y el entrenamiento\n",
        "    K_train_train = np.matmul(np.matmul(phi_t,eps),phi_t.T) # Es la matriz de covarianzas entre los datos de entrenamiento\n",
        "\n",
        "    return K_prior,K_pred_train,K_train_train, phi_x, phi_t, np.zeros(x.size)\n",
        "\n",
        "  def covarianzaProceso(self,phi_x,covM):\n",
        "    '''\n",
        "    Devuelve la matriz de covarianza correspondiente a distribucion de probabilidad a priori en base a la incertidumbre de los parametros de una regresion lineal .\n",
        "    '''\n",
        "    return np.matmul(np.matmul(phi_x,covM),phi_x.T)\n",
        "\n",
        "  def covarianzaAleatoria(self,x, x_train,sigma):\n",
        "    '''\n",
        "    Genera matriz de covarianza con correlaciones aleatorias sobre el espacio en que realizamos el proceso gaussiano,\n",
        "    generamos una matriz de la aplicacion aleatoria dada una distribucion normal Sigma ~ normal(0,1)\n",
        "\n",
        "    Partimos de que a priori, sin condicionar resultados, si tenemos los puntos sobre los que samplear las funciones\n",
        "    y los de entrenamiento, partimos de que la matriz de covarianza a priori total o global resulta de combinar las siguientes submatrices\n",
        "\n",
        "                                                  ( M(n_star,n_star), M(n_star,n_train) \n",
        "    M(n_star + n_train, n_star + n_train) =    (                                          )\n",
        "                                                  ( M(n,train,n_star), M(n_train,n_train)\n",
        "\n",
        "    Que es generada recordando la siguiente propiedad de matrices definidas positivas simétricas\n",
        "\n",
        "    Cov = M*M_t\n",
        "\n",
        "    Simplemente debemos generar M y obtener la matriz de covarianza de la anterior forma y las submatrices de covarianza cuando queramos condicionar el procesos gausiano\n",
        "\n",
        "    Creo que no importa el orden de las coordenadas porque estamos trabajando con algebra lineal (permutaciones de filas dejan la matriz con las mismas propiedades)\n",
        "\n",
        "    '''\n",
        "\n",
        "    phi_x = x\n",
        "    phi_t = x_train(97,)\n",
        "    sqrt_K_total = np.array([[random.gauss(0,0.1) for xi in np.concatenate((x, x_train), axis=None)] for xj in np.concatenate((x, x_train), axis=None)])\n",
        "\n",
        "    K_total = np.matmul(sqrt_K_total,sqrt_K_total.T) # Definiendola así nos aseguramos que resulte una matriz definida positiva\n",
        "   \n",
        "    K_prior = K_total[:x.size,:x.size]\n",
        "    K_pred_train = K_total[:x.size,x.size:]\n",
        "    K_train_train = K_total[x.size:,x.size:]\n",
        "\n",
        "    return K_prior,K_pred_train,K_train_train, phi_x, phi_t, np.zeros(x.size)\n",
        "\n",
        "\n",
        "\n",
        "  def corr_linear_creciente(self,x, x_train,s,l):\n",
        "    '''\n",
        "    Genera matriz de covarianza con correlaciones que aumentan con la separación entre coordenadas\n",
        "    sigma y l son hiperparámetros de este kernel\n",
        "    '''\n",
        "\n",
        "  \n",
        "    phi_x = x\n",
        "    phi_t = x_train\n",
        "\n",
        "    x_domain = np.concatenate((x,x_train), axis =None)\n",
        "\n",
        "    K_total =  np.array([[(s*np.abs(xj-xi))/l for xi in x_domain] for xj in x_domain]) # Debemos tomar valor absoluto para asegurarnos que la matriz resultante sea simétrica\n",
        "\n",
        "    #K_total = np.matmul(sqrt_K_total,sqrt_K_total.T) # Definiendola así nos aseguramos que resulte una matriz definida positiva    \n",
        "\n",
        "    K0, K1, K2 = self.submatrices(K_total, x.size, x_train.size)\n",
        "\n",
        "    return K0, K1, K2, phi_x, phi_t, np.zeros(x.size)\n",
        "\n",
        "  def corr_linear_decreciente(self,x, x_train,s,l):\n",
        "    '''\n",
        "    Genera matriz de covarianza con correlaciones que disminuyen con la separación entre coordenadas\n",
        "    sigma y l son hiperparámetros de este kernel\n",
        "\n",
        "    cor(dx) =   s. / (1 + dx/l)\n",
        "\n",
        "    '''\n",
        "\n",
        "  \n",
        "    phi_x = x\n",
        "    phi_t = x_train\n",
        "\n",
        "    x_domain = np.concatenate((x,x_train), axis =None)\n",
        "\n",
        "    K_total =  np.array([[s/(1.+(np.abs(xj-xi)/l)) for xi in x_domain] for xj in x_domain]) # Debemos tomar valor absoluto para asegurarnos que la matriz resultante sea simétrica\n",
        "\n",
        "    #K_total = np.matmul(sqrt_K_total,sqrt_K_total.T) # Definiendola así nos aseguramos que resulte una matriz definida positiva    \n",
        "\n",
        "    K0, K1, K2 = self.submatrices(K_total, len(x), len(x_train))\n",
        "\n",
        "    return K0, K1, K2, phi_x, phi_t, np.zeros(len(x))\n",
        "\n",
        "\n",
        "  def kernel_exponencial_cuadratico(self,x, x_train,s,l):\n",
        "    '''\n",
        "    Genera matriz de covarianza con correlaciones de acuerdo al kernel exponencial cuadrático\n",
        "\n",
        "    K(x-x') =   s² exp((-[x-x']²)/2l²)\n",
        "\n",
        "    '''\n",
        "\n",
        "    x_domain = np.concatenate((x_train,x), axis =None)\n",
        "\n",
        "    K_total =  np.array([[np.power(s,2)*np.exp(-np.power(xi-xj,2)/(2.*np.power(l,2)))  for xi in x_domain] for xj in x_domain]) # Debemos tomar valor absoluto para asegurarnos que la matriz resultante sea simétrica\n",
        "\n",
        "\n",
        "    # Se calculan derivadas parciales respecto los hiperparámetros del modelo\n",
        "\n",
        "    #s\n",
        "    K_total_part_d_s = np.array([[2.*s*np.exp(-np.power(xi-xj,2)/(2.*np.power(l,2)))  for xi in x_domain] for xj in x_domain]) \n",
        "    \n",
        "    #l\n",
        "    K_total_part_d_l = np.array([[-(np.power(xi-xj,2)/np.power(l,3))*np.power(s,2)*np.exp(-np.power(xi-xj,2)/(2.*np.power(l,2)))  for xi in x_domain] for xj in x_domain]) \n",
        "\n",
        "\n",
        "    K0, K1, K2 = self.submatrices(K_total, len(x_train),len(x_train))\n",
        "\n",
        "    K_p_s_0, K_p_s_1, K_p_s_2 = self.submatrices(K_total_part_d_s, len(x_train),len(x_train))\n",
        "    K_p_l_0, K_p_l_1, K_p_l_2 = self.submatrices(K_total_part_d_l, len(x_train),len(x_train))\n",
        "\n",
        "\n",
        "    return K2, K1.T, K0,  K_p_s_0, K_p_s_1, K_p_s_2, K_p_l_0, K_p_l_1, K_p_l_2, x, x_train, np.zeros(len(x))\n",
        "\n",
        "  def submatrices(self,K_total,i,f):\n",
        "    '''\n",
        "    Este metodo, dada una matriz de covarianzas del proceso gausiano sin condicionar a las observaciones,\n",
        "    devuelve las matrices K_prior, K_pred_train, K_train_train que serán usadas para muestrear el proceso\n",
        "\n",
        "    Solo sirve si el dominio de prueba y el de entrenamiento estan concatenados secuencialmente (no mezclados u ordenados de otra manera los puntos)\n",
        "    '''\n",
        "    Ks = [K_total[:i,:i], K_total[:i,i:], K_total[i:,i:]]\n",
        "    return Ks[0], Ks[1], Ks[2] #  K_train_train, K_train_pred, K_pred_pred o K_prior prior respectivamente\n",
        "\n",
        "\n",
        "\n",
        "class gaussProcess():\n",
        "  def __init__(self,K_p,K_p_t,K_t_t,prior_Partial_Der,Mean, Input, noiseLevel):\n",
        "    '''\n",
        "    Los objetos de esta clase modelan procesos gaussianos caracterizados por su promedio y covarianza\n",
        "    Eps es la matriz de covarianzas de los pesos del proceso gausiano\n",
        "    prior_Partial_der corresponde a una lista de matrices de la distribución a priori correspondientes a su derivada respecto los distintos parámetros\n",
        "    '''\n",
        "    self.x = Input # Dominio de la regresion\n",
        "    self.K_prior = K_p # Matriz de covarianzas usada para definir la distribucion a priori y mas tarde para muestrear funciones del proceso gausiano\n",
        "    self.K_pred_train = K_p_t # Es la matriz de covarianzas entre datos de la distribución a priori y el entrenamiento\n",
        "    self.K_train_train = K_t_t # Es la matriz de covarianzas entre los datos de entrenamiento\n",
        "    self.mean_prior = Mean\n",
        "    self.Derivadas = prior_Partial_Der\n",
        "    self.L_prior = self.cholDescomp(self.K_prior)\n",
        "    self.Ruido = noiseLevel  # amplitud del ruido\n",
        "\n",
        "    class covarianzas():\n",
        "      def __init__(self,modelos):\n",
        "        self.covarianzas_usadas = modelos # Lista \n",
        "\n",
        "  def cholDescomp(self,K):\n",
        "    '''\n",
        "    Cholesky decomposition\n",
        "    '''\n",
        "    try:\n",
        "      L = scipy.linalg.cholesky(K, lower=True)\n",
        "    except:\n",
        "      L = scipy.linalg.cholesky(K + np.diag(0.000001*np.ones(int(np.sqrt(K.size)))), lower=True)\n",
        "\n",
        "    return L\n",
        "\n",
        "  def calcInvK(self,matriz):\n",
        "    '''\n",
        "    Calculo de la inversa de matriz. Pensado para ser la inversa de la matriz de covarianza.\n",
        "    '''\n",
        "    # Calculo de matriz inversa de la covarianza. \n",
        "    try:\n",
        "      self.invK = np.linalg.inv(matriz)\n",
        "    except:\n",
        "      print(\"La matriz inversa no existe porque el determinante es cero\")\n",
        "\n",
        "\n",
        "\n",
        "  def condicionarGP(self,entrenamiento_x,entrenamiento_y):\n",
        "    '''\n",
        "    Dadas unas observaciones junto a sus valores en el dominio de los procesos, obtiene la distribución condicionada a las trayectorias asociadas a dichas medidas.\n",
        "    También calcula el logaritmo de la probabilidad de verosimilitud marginal (marginal likelihood) de las observaciones dados los inputs \n",
        "    y parámetros del modelo usado.\n",
        "\n",
        "    Este algoritmo esta especificado en el libro \"Gaussian Processes for Machine Learning\", como algoritmo 2.1\n",
        "    '''\n",
        "    self.xtrain = entrenamiento_x\n",
        "    self.ytrain = entrenamiento_y\n",
        "    self.n_train = len(entrenamiento_x)\n",
        "\n",
        "    # Definimos nueva covarianza\n",
        "    # Primero se realiza la descomposición cholesky de la adición de la covarianza de la distribución a priorir y el término de ruido.\n",
        "    cholL = np.linalg.cholesky(self.K_train_train + np.power(self.Ruido,2)*np.diag(np.ones(len(self.xtrain))))\n",
        "\n",
        "    # Calculo del vector alfa como solucion del sistema K*alfa = y_entrenamiento usando la descomposición cholesky anterior\n",
        "    alfa = scipy.linalg.cho_solve((cholL,True),self.ytrain)\n",
        "\n",
        "    # La media predictiva dados los datos\n",
        "    self.mean_pred = np.matmul(self.K_pred_train,alfa)\n",
        "\n",
        "    # Vector v solución particular del sistema cholL*v = k_star para todos los datos del entrenamiento.\n",
        "    # Este sistema se resuelve introduciendo k_star no como matriz columna sino como la submatriz calculada como la evaluación de la covarianza a priori de los inputs de los valores a predecir y los inputs del entrenamiento\n",
        "    v = scipy.linalg.solve_triangular(cholL,self.K_pred_train.T, lower=True)\n",
        "\n",
        "\n",
        "    # Covarianza predictiva. El proceso llevado a cabo ha sido obtener la distribución condicionada del GP (inputs predictivos) a las observaciones y_entrenamiento\n",
        "    self.K_pred = self.K_prior - np.matmul(v.T,v)\n",
        "\n",
        "    # Magnitud importante para muestrear procesos gausianos.\n",
        "    self.L_pred = self.cholDescomp(self.K_pred)\n",
        "\n",
        "\n",
        "  def log_prob_verosimilitud_datos(self,entrenamiento_x,entrenamiento_y):\n",
        "    '''\n",
        "    Dadas unas observaciones, kernel e hiperparámetros dados, calcula el logaritmo de la probabilidad de verosimilitud de las observaciones al modelo y sus parámetros\n",
        "    '''\n",
        "\n",
        "    # Primero se realiza la descomposición cholesky de la adición de la covarianza de la distribución a priorir y el término de ruido.\n",
        "    cholL = np.linalg.cholesky(self.K_train_train + np.power(self.Ruido,2)*np.diag(np.ones(len(entrenamiento_x))))\n",
        "\n",
        "    # Calculo del vector alfa como solucion del sistema K*alfa = y_entrenamiento usando la descomposición cholesky anterior\n",
        "    alfa = scipy.linalg.cho_solve((cholL,True),entrenamiento_y)\n",
        "\n",
        "    # Calculo de la probabilidad de verosimilitud marginal a este modelo y sus parámetros\n",
        "    self.log_marg_y = -np.sum(np.log(np.diag(cholL)),axis=0)\n",
        "    self.log_marg_y = self.log_marg_y - 0.5*np.dot(alfa,entrenamiento_y)\n",
        "    self.log_marg_y = self.log_marg_y - (float(len(entrenamiento_x)/2.))*np.log(2*pi)\n",
        "\n",
        "\n",
        "  def distribucionGP(self,mean,L):\n",
        "    '''\n",
        "    Genera realizaciones del proceso gaussiano dada la descomposicion cholesky de la matriz de covarianzas y la media correspondiente\n",
        "    '''\n",
        "    gaussNumbers = np.fromiter([ random.gauss(0,1) for x in range(len(np.diag(L))) ],float)\n",
        "    return mean + np.matmul(L,gaussNumbers.T)\n",
        "\n",
        "  def sigmaCalc(self,cov):\n",
        "    '''\n",
        "    Desviacion estándar punto a punto del proceso gausiano dado por la matriz de covarianza \n",
        "    Es la raiz cuadrado de los elementos de la diagonal de la matriz de covarianzas del proceso generado\n",
        "    '''\n",
        "    return np.sqrt(np.diag(cov))\n",
        "\n",
        "\n",
        "  def correlacionPuntoPunto(self):\n",
        "    '''\n",
        "    Obtiene la correlacion punto a punto dado un inputo concreto por el indice i_x\n",
        "    '''\n",
        "    try:\n",
        "      self.corrPP_pred = np.array([ self.K_pred[i,:] for i in range(len(self.K_pred)) ])\n",
        "    except:\n",
        "      print(\" Proceso gaussiano no entrenado aún, no existe K_pred \")\n",
        "    self.corrPP_prior = np.array([ self.K_prior[i,:] for i in range(len(self.K_prior)) ])\n",
        "\n",
        "  def derivadaVerosimilitudMarginal(self,*args):\n",
        "    '''\n",
        "    Dadas unas observaciones particulares estima la derivada del logaritmo de la probabilidad asociada a la distribución de verosimilitud marginal asociada al modelo respecto sus hiperparámetros\n",
        "    '''\n",
        "\n",
        "    if not isinstance(args[0], np.ndarray):\n",
        "      raise NameError(\"Las observaciones dadas para calcular la derivada del logaritmo de la distribución marginal de verosimilitud no es un array con datos\")\n",
        "    else:\n",
        "      observaciones = args[0]\n",
        "      observaciones = observaciones.reshape(len(observaciones),1) # Esta operacion es necesaria para que mas tarde el producto matmul nos reproduzca una matriz en lugar de producto escalar entre vectores.\n",
        "\n",
        "    # Vector auxiliar alpha\n",
        "    try:\n",
        "      alpha = np.matmul(self.invK,observaciones)\n",
        "    except:\n",
        "      raise NameError(\"Necesitas calcular la matriz inversa de covarianzas de la distribución a priori\")\n",
        "\n",
        "    alpha_alpha = np.matmul(alpha,alpha.T) \n",
        "    alpha_alpha_minus_invK = alpha_alpha - self.invK\n",
        "\n",
        "    # Calculo. Es el valor de la derivada del logaritmo\n",
        "    var_incSignal = self.Ruido*np.sum(alpha_alpha_minus_invK)# Si asumimos como hiperparámetro la amplitud del ruido sobre las medidas respecto la señal verdadera, se debe añadir este término\n",
        "    self.derVerMarg = [ 0.5*np.trace(np.matmul(alpha_alpha_minus_invK,self.Derivadas[i])) for i in range(len(self.Derivadas))]\n",
        "    self.derVerMarg.append(var_incSignal)\n",
        "    self.derVerMarg = np.asarray(self.derVerMarg)\n",
        "  \n",
        "  def derivada_LO_CV(self,*args):\n",
        "    '''\n",
        "    Dadas unas observaciones particulares estima la derivada de la suma de los logaritmos de la evaluación de la distribución de probabilidad marginal de verosimilitud a que subconjuntos\n",
        "    de las observaciones están condicionadas al resto, respecto a los hiperparámetros del modelo. Si se encuentra el máximo de este valor, estamos asegurándonos de que con los hiperparámetros ajustados\n",
        "    las \"predicciones\" tras generar trayectorias del proceso gausiano que corresponderían a los subconjuntos excluidos sucesivamente son las más probables que el modelo puede hacer. Al ser la suma, esta optimización \n",
        "    es global y abarca la aproximación de observacion/test de todos los subconjuntos.\n",
        "    '''\n",
        "\n",
        "    if not isinstance(args[0], np.ndarray):\n",
        "      raise NameError(\"Las observaciones dadas para calcular la derivada del logaritmo de la distribución marginal de verosimilitud no es un array con datos\")\n",
        "    else:\n",
        "      observaciones = args[0]\n",
        "\n",
        "    # Vector auxiliar alpha y matriz auxiliar Z y la diagonal de la inversa de la distribucion a priori\n",
        "    try:\n",
        "      alpha = np.matmul(self.invK,observaciones)\n",
        "      invK_diag = np.diag(self.invK)\n",
        "      zeta = [np.matmul(self.invK,self.Derivadas[i]) for i in range(len(self.Derivadas)) ] # Es una lista de matrices\n",
        "      zeta_K = [np.diag(np.matmul(zeta[i],self.invK)) for i in range(len(self.Derivadas)) ]\n",
        "      alpha_invK = np.multiply(alpha,1./invK_diag)\n",
        "    except:\n",
        "      raise NameError(\"Necesitas calcular la matriz inversa de covarianzas de la distribución a priori\")\n",
        "    \n",
        "    termino_1 = [np.matmul(np.matmul(alpha_invK,zeta[i]),alpha) for i in range(len(self.Derivadas)) ] # Debido a Z esto es una lista\n",
        "    termino_2 = [np.sum(np.multiply(np.multiply(0.5*(1. + np.multiply(np.power(alpha,2),1./invK_diag)),zeta_K[i]),1./invK_diag)) for i in range(len(self.Derivadas)) ] # Debido a zeta_K esto es una lista\n",
        "\n",
        "    # Calculo. Es el valor de la derivada del logaritmo\n",
        "    self.derVerLOO = np.asarray([ termino_1[i] + termino_2[i] for i in range(len(self.Derivadas))])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Ogka96GyNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class datosVisualizacion():\n",
        "  '''\n",
        "  Esta clase contiene los datos de entrenamiento y realizaciones de los procesos gaussianos para la visualización de los mismos\n",
        "  '''\n",
        "  def __init__(self,Titulo):\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "    self.sigma = []\n",
        "    self.realizacion_dict = {}\n",
        "    self.titulo = Titulo\n",
        "    self.inicializacionPlot()\n",
        "\n",
        "  def inicializacionPlot(self):\n",
        "    '''\n",
        "    Inicializa el objeto matplotlib\n",
        "    '''\n",
        "    self.fig,self.ax = plt.subplots(nrows=1,ncols=1,figsize=(10,10))\n",
        "    self.ax.set_title(self.titulo)\n",
        "    self.fig_hm, self.ax_hm = plt.subplots(nrows=1,ncols=1,figsize=(10,10))\n",
        "\n",
        "  def add_covariance(self, cov):\n",
        "    '''\n",
        "    Añada matriz de covarianza y alternativamente calcula la covarianza del proceso gaussiano sobre el dominio\n",
        "    '''\n",
        "\n",
        "    self.cov_GP = cov\n",
        "\n",
        "  def add_mean(self,X,Mean):\n",
        "    '''\n",
        "    Almacena la desviación estándar de un proceso gausiano dado\n",
        "    '''\n",
        "    self.x_mean = X\n",
        "    self.mean = Mean\n",
        "\n",
        "\n",
        "  def add_sigma(self,X,Sigma):\n",
        "    '''\n",
        "    Almacena la desviación estándar de un proceso gausiano dado\n",
        "    '''\n",
        "    self.x_sigma = X\n",
        "    self.sigma = Sigma\n",
        "    self.meanMarker = 'r-'\n",
        "\n",
        "  def add_train_data(self,X,Y):\n",
        "    self.xtrain = X\n",
        "    self.ytrain = Y\n",
        "    self.marker = 'b*'\n",
        "\n",
        "  def add_data(self,X,Y,label):\n",
        "    self.x.append(X)\n",
        "    self.y.append(Y)\n",
        "    self.realizacion_dict[label] = len(self.x)-1\n",
        "\n",
        "  def add_corrPuntoPunto(self,corrPP, corrInputs):\n",
        "    '''\n",
        "    Añade correlaciones punto a punto sobre un dominio dado por los valores corrInputs\n",
        "    '''\n",
        "    self.corrPP = corrPP\n",
        "    self.corr_in = corrInputs\n",
        "\n",
        "  def add_data_plot(self,label):\n",
        "    try:\n",
        "      self.ax.plot(self.x[self.realizacion_dict[label]],self.y[self.realizacion_dict[label]],'g--')\n",
        "    except:\n",
        "      print('No existe datos referentes a etiqueta :',label)\n",
        "\n",
        "  def add_train_data_plot(self):\n",
        "    try:\n",
        "      self.ax.plot(self.xtrain,self.ytrain,self.marker)\n",
        "    except:\n",
        "      print('No existen datos de entrenamiento')\n",
        "\n",
        "  def add_mean_plot(self):\n",
        "    try:\n",
        "      self.ax.plot(self.x_mean,self.mean,self.meanMarker)\n",
        "    except:\n",
        "      print('No existen datos de entrenamiento')\n",
        "\n",
        "\n",
        "\n",
        "  def contorno_sigma(self):\n",
        "    '''\n",
        "    Muestra en pantalla contorno de 1 y 2 veces los valores de sigma del proceso gausiano\n",
        "    '''\n",
        "    self.ax.fill(np.concatenate((self.x_sigma,self.x_sigma[::-1])),\n",
        "             np.concatenate((self.mean - 1.9600 * self.sigma,\n",
        "                             (self.mean + 1.9600 * self.sigma)[::-1])),\n",
        "             alpha=.45, fc='y', ec='None', label='95% confidence interval')\n",
        "    self.ax.fill(np.concatenate((self.x_sigma,self.x_sigma[::-1])),\n",
        "             np.concatenate((self.mean - 1.000 * self.sigma,\n",
        "                             (self.mean + 1.000 * self.sigma)[::-1])),\n",
        "             alpha=.35, fc='b', ec='None', label='68% confidence interval')\n",
        "    \n",
        "  def plotCorrPuntoPunto(self,indices):\n",
        "    '''\n",
        "    Visualiza correlaciones punto a punto para los inputs dados por sus indices en la lista indices como argumento del metodo\n",
        "    '''\n",
        "\n",
        "    ' Generamos y visualizamos las imagen directamente en este metodo'\n",
        "    self.fig_cpp, self.ax_cpp = plt.subplots(nrows=1,ncols=1,figsize=(10,10))\n",
        "    try:\n",
        "      [self.ax_cpp.plot(self.corr_in,self.corrPP[indice],'-') for indice in indices] \n",
        "    except:\n",
        "      print('No existe datos referentes referentes a correlaciones punto a punto')\n",
        "    self.ax_cpp.grid(True)\n",
        "    self.fig_cpp.show()\n",
        "\n",
        "\n",
        "\n",
        "  def mostrarPlot(self):\n",
        "    '''\n",
        "    Muestra plot\n",
        "    '''\n",
        "    self.add_mean_plot()\n",
        "    self.add_train_data_plot()\n",
        "    self.contorno_sigma()\n",
        "    self.ax.grid(True)\n",
        "    self.fig.show()\n",
        "\n",
        "  def hmapCov(self):\n",
        "    '''\n",
        "    Visualiza covarianza como campo bidimensional\n",
        "    '''\n",
        "    plotRange = np.arange(len(self.cov_GP))\n",
        "    heatmap = self.ax_hm.pcolormesh(self.cov_GP,cmap='RdBu_r')\n",
        "    self.ax_hm.axis([plotRange.min(), plotRange.max(), plotRange.max(), plotRange.min()])\n",
        "    self.fig_hm.colorbar(heatmap, ax=self.ax_hm)\n",
        "    self.fig_hm.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}