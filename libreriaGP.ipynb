{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPLJ06vNgpkmwpU0IFqE/HC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanlu29/juanlu29/blob/gp_aprendizaje/libreriaGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juguGpOpG3HF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modulos y constantes\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy\n",
        "import scipy.linalg\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numba"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djR_jiyCGlbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class gaussProcess():\n",
        "  def __init__(self,dominio,observacion,*hiper):\n",
        "    '''\n",
        "    Los objetos de esta clase modelan procesos gaussianos caracterizados por su promedio y covarianza\n",
        "    Como argumento inicial incluimos el data set sobre el que se va a trabajar, su dominio y las observaciones como ndarrays\n",
        "    '''\n",
        "    #self.Derivadas = prior_Partial_Der\n",
        "    self.Kernels = self.kernels()\n",
        "    self.Dominio = dominio\n",
        "    self.Observacion = observacion\n",
        "    self.Hiper = hiper\n",
        "    # Se crean matrices [[x],[x],...] (longitud de la concatenacion nx) y [[y],[y],...].T (longitud de la concatenacion ny)\n",
        "    # Son necesarios para devolver los resultados necesarios en los calculos matriciales\n",
        "    self.xM = np.full((len(self.Observacion),len(self.Dominio)),self.Dominio)\n",
        "    self.yM = (np.full((len(self.Dominio),len(self.Observacion)),self.Observacion))\n",
        "\n",
        "  def calcCovM(self,xM,yM):\n",
        "    '''\n",
        "    Dados x e y como ndarrays, calcula la matriz de covarianzas cuyos elementos son evaluados de la forma\n",
        "    C(xi,yi) = (k(xi,yi)) con rango (nx x ny) donde nx y ny son la longitud de los vectores x  e y respectivamente.\n",
        "    hiper son los hiperparámetros usados en el modelo\n",
        "    '''\n",
        "    # Llamamos la funcion Ker que nos debe devolver como salida una matriz cuyos elementos calculados representan k(x,y)\n",
        "    return self.Ker(xM,yM.T,*self.Hiper)\n",
        "\n",
        "  def calcDerCovM(self,xM,yM):\n",
        "    '''\n",
        "    Dados x e y como ndarrays, calcula la matriz correspondiente a la derivada respecto los hiperparámetros de las covarianzas\n",
        "    cuyos elementos son evaluados de la forma C_j(xi,yi) = (dk(xi,yi)dh_j) con rango (nx x ny) donde nx y ny son la longitud de los \n",
        "    vectores x e y respectivamente. Hiper son los hiperparámetros usados en el modelo\n",
        "    '''\n",
        "    # Generamos el array de matrices llamando al zip de las funciones de derivadas y sus hiperparámetros para generarlas\n",
        "    return np.asarray([ self.DerKer[i](xM,yM.T,*self.Hiper) for i in range(len(self.DerKer)) ] )\n",
        "\n",
        "  def compCov(self,covarianza):\n",
        "    '''\n",
        "    Permite definir el kernel a usar por el proceso gaussiano, tanto si es uno de los kernels definidos en la clase como si es una combinacion de los mismos\n",
        "    El argumento es una funcion anónima en caso de ser una composición, en caso contrario es suficiente con proporcionar el nombre del kernel definido\n",
        "    hiper son los hiperparámetros que precisan el kernel y vienen dado por un diccionario\n",
        "    '''\n",
        "    self.Ker = covarianza # Covarianza definida\n",
        "    self.Ker = np.vectorize(self.Ker)\n",
        "\n",
        "  def compDerCov(self,derCovarianza):\n",
        "    '''\n",
        "    Permite definir la expresion para la derivada del kernel respecto los distintos hiperparametros como un iterable zip\n",
        "    de funciones anonimas juntos a sus hiperparámetros. \n",
        "    En este caso por propositos de optimizacion se espera que se evalue dinamicamente con distintos hiperparámetros.\n",
        "    Los datos contenidos en *hiper están encapsulados en una tupla a la que se debe acceder previamente\n",
        "    '''\n",
        "    try:\n",
        "      self.DerKer = derCovarianza\n",
        "    except:\n",
        "      raise NameError(\"No se ha definido kernel previamente\")\n",
        "\n",
        "\n",
        "  class kernels():\n",
        "    '''\n",
        "    Se definen kernels de uso común junto a sus hiperparámetros y derivadas asociadas\n",
        "    '''\n",
        "    def __init__(self):\n",
        "      return\n",
        "\n",
        "    def exponencialCuadrada(self,xb,xa,s,l):\n",
        "      '''\n",
        "      Correlacion estacionaria (depende solo de la distancia r = abs(xb-xa)) con decaimiento exponencial cuadratico\n",
        "\n",
        "      k(xb,xa) = s² exp( - (xb-xa)²/(2 l²) )\n",
        "\n",
        "      Hiperparámetros:\n",
        "      s : amplitud de la correlacion, esta relacionado con la amplitud de las trayectorias generadas por el GP\n",
        "      l : escala de longitud del GP. Esta relacionada con la distancia sobr la cual los procesos generados por el GP pueden presentar oscilaciones o variaciones\n",
        "      '''\n",
        "      return np.power(s,2)*np.exp(-np.power(xa-xb,2)/(2.*np.power(l,2)))\n",
        "\n",
        "    def expCua_der_s(self,xb,xa,s,l):\n",
        "      '''\n",
        "      Derivada respecto el hiperparámetro s de las correlaciones estacionarias con decaimiento exponenciales cuadráticas\n",
        "\n",
        "      k(xb,xa) = 2s exp( - (xb-xa)²/(2 l²) )\n",
        "\n",
        "      Hiperparámetros:\n",
        "      s : amplitud de la correlacion, esta relacionado con la amplitud de las trayectorias generadas por el GP\n",
        "      l : escala de longitud del GP. Esta relacionada con la distancia sobr la cual los procesos generados por el GP pueden presentar oscilaciones o variaciones\n",
        "      '''\n",
        "      return 2*s*np.exp(-np.power(xa-xb,2)/(2.*np.power(l,2)))\n",
        "\n",
        "    def expCua_der_l(self,xb,xa,s,l):\n",
        "      '''\n",
        "      Derivada respecto el hiperparámetro l de las correlaciones estacionarias con decaimiento exponenciales cuadráticas\n",
        "\n",
        "      k(xb,xa) = s²(xb-xa)² / l³ exp( - (xb-xa)²/(2 l²) )\n",
        "\n",
        "      Hiperparámetros:\n",
        "      s : amplitud de la correlacion, esta relacionado con la amplitud de las trayectorias generadas por el GP\n",
        "      l : escala de longitud del GP. Esta relacionada con la distancia sobr la cual los procesos generados por el GP pueden presentar oscilaciones o variaciones\n",
        "      '''\n",
        "      return (np.power(s*(xa-xb),2)/np.power(l,3))*np.exp(-np.power(xa-xb,2)/(2.*np.power(l,2)))\n",
        "\n",
        "    def ruidoBlanco(self,xb,xa,sigma):\n",
        "      '''\n",
        "      Correlaciones de ruido blanco o gaussiano puro, solo correlaciona los puntos consigo mismo. \n",
        "\n",
        "      k(xb,xa) = sigma² delta(xb,xa)\n",
        "\n",
        "      Representa incertidumbres intrínsecas en las observaciones y asume una distribución a priori de esos errores gaussiano\n",
        "      Hiperparáámetros:\n",
        "      s : amplitud o incertidumbre del ruido generado\n",
        "      '''\n",
        "      return np.where(xb==xa, np.power(sigma,2),0.) # Se usa el metodo numpy.where porque las operaciones logicas sobre arrays son consideradas ambiguas en el intérprete de python\n",
        "                                                    # Esta manera es válidad para python para comprar elemento a elemento entre matrices.\n",
        "\n",
        "    def rB_der_sigma(self,xb,xa,sigma):\n",
        "      '''\n",
        "      Derivada respecto el hiperparámetro sigma del ruido no correlacionado\n",
        "\n",
        "      k(xb,xa) = sigma² delta(xb,xa)\n",
        "\n",
        "      Representa incertidumbres intrínsecas en las observaciones y asume una distribución a priori de esos errores gaussiano\n",
        "      Hiperparáámetros:\n",
        "      s : amplitud o incertidumbre del ruido generado\n",
        "      '''\n",
        "      return np.where(xb==xa, 2*sigma,0.) # Se usa el metodo numpy.where porque las operaciones logicas sobre arrays son consideradas ambiguas en el intérprete de python\n",
        "                                          # Esta manera es válidad para python para comprar elemento a elemento entre matrices.\n",
        "\n",
        " \n",
        "  def cholDescomp(self,K):\n",
        "    '''\n",
        "    Descomposicion cholesky de una matriz dada\n",
        "    '''\n",
        "    try:\n",
        "      L = scipy.linalg.cholesky(K, lower=True)\n",
        "    except:\n",
        "     # L = scipy.linalg.cholesky(K + np.diag(0.000001*np.ones(int(np.sqrt(K.size)))), lower=True)\n",
        "      raise NameError(\"No es posible efectuar descomposicion cholesky\")\n",
        "\n",
        "    return L\n",
        "\n",
        "  def calcInvK(self,matriz):\n",
        "    '''\n",
        "    Calculo de la inversa de matriz. \n",
        "    '''\n",
        "    # Calculo de matriz inversa de la covarianza. \n",
        "    try:\n",
        "      invK = np.linalg.inv(matriz)\n",
        "    except:\n",
        "      raise NameError(\"No es posible calcular matriz inversa\")\n",
        "\n",
        "    return invK\n",
        "\n",
        "  def condicionarGP(self,*dominio_test):\n",
        "    '''\n",
        "    Dadas unas observaciones junto a sus valores de dominio (uni-dimensionales) en el rango sobre el que se modela el GP, \n",
        "    obtiene la covarianza asociada a la distribución de trayectorias condicionadas al conjunto de partida dado un dominio \n",
        "    de test en el que queremos comprobar la predicción. Por tanto permite generar las trayectorias compatibles con los datos\n",
        "\n",
        "    También calcula el logaritmo de la probabilidad de verosimilitud marginal (marginal likelihood) de las observaciones dados los inputs \n",
        "    y parámetros del modelo usado.\n",
        "\n",
        "    Este algoritmo esta especificado en el libro \"Gaussian Processes for Machine Learning\", como algoritmo 2.1\n",
        "    '''\n",
        "    self.xtest = np.reshape(dominio_test,(len(dominio_test),1))\n",
        "\n",
        "    # Definimos nueva covarianza\n",
        "    # Primero se realiza la descomposición cholesky de la adición de la covarianza de la distribución a priori,\n",
        "    # para lo cual calculamos la matriz de covarianza asociada al dominio de las observaciones sobre si mismo\n",
        "    # y al cruce del dominio de las observaciones y del test de prediccion.\n",
        "    try:\n",
        "      self.Cov_obs_obs = self.calcCovM(self.xM,self.xM,*self.Hiper)\n",
        "      self.Cov_obs_test = self.calcCovM(self.xM,self.xtest,*self.Hiper)\n",
        "    except:\n",
        "      raise NameError(\"Debes especificar primero el kernel que estas usando junto a sus hiperparametros con el metodo self.compCov\")\n",
        "\n",
        "    cholL = np.linalg.cholesky(self.Cov_obs_obs)\n",
        "\n",
        "    # Calculo del vector alfa como solucion del sistema K_obs_obs*alfa = y_entrenamiento usando la descomposición cholesky anterior\n",
        "    self.Alfa = scipy.linalg.cho_solve((cholL,True),self.yM)\n",
        "\n",
        "    # Vector v solución particular del sistema cholL*v = k_test para todos los datos del entrenamiento.\n",
        "    # Este sistema se resuelve introduciendo k_star no como matriz columna sino como la submatriz calculada como la evaluación de la covarianza a priori de los inputs de los valores a predecir y los inputs del entrenamiento\n",
        "    v = scipy.linalg.solve_triangular(cholL,self.cov_obs_test, lower=True)\n",
        "\n",
        "    # Se obtiene el valor promedio del proceso predictivo condicionado a las observaciones\n",
        "    self.Media_pred = np.matmul(self.Cov_obs_test.T,self.Alfa)\n",
        "\n",
        "    # Covarianza predictiva o condicionada. El proceso llevado a cabo ha sido obtener la distribución condicionada del GP a las observaciones\n",
        "    # Intenta calcular directamente la matriz, en caso contrario calcular la matriz de covarianza de la distribucion a priori dado el test\n",
        "    try:\n",
        "      self.Cov_pred = self.Cov_priori - np.matmul(v.T,v)\n",
        "    except:\n",
        "      # En caso que no esté definida o no coincida con la dimensionalidad del test, se recalcula la covarianza a priori\n",
        "      self.Cov_priori = self.calcCovM(self.xtest,self.xtest,*self.Hiper)\n",
        "      self.Cov_pred = self.Cov_priori - np.matmul(v.T,v)\n",
        "\n",
        "    # La siguiente matriz descompuesta es necesaria para generar procesos del GP.\n",
        "    self.L_pred = self.cholDescomp(self.Cov_pred)\n",
        "  \n",
        "\n",
        "  def log_prob_verosimilitud_datos(self,*hiper):\n",
        "    '''\n",
        "    Dadas unas observaciones, kernel e hiperparámetros dados, calcula el logaritmo de la probabilidad de verosimilitud de las observaciones al modelo y sus parámetros\n",
        "    '''\n",
        "\n",
        "    # Actualizamos hiperparámetros\n",
        "    self.Hiper = hiper\n",
        "\n",
        "    # Primero se realiza la descomposición cholesky de la adición de la covarianza de la distribución a priorir y el término de ruido.\n",
        "    #cholL = np.linalg.cholesky(self.calcCovM(dominio,dominio,*self.Hiper))\n",
        "    cholL = np.linalg.cholesky(self.calcCovM(self.xM,self.xM))\n",
        "\n",
        "    # Calculo del vector alfa como solucion del sistema K*alfa = y_entrenamiento usando la descomposición cholesky anterior\n",
        "    alfa = scipy.linalg.cho_solve((cholL,True),self.Observacion)\n",
        "\n",
        "    # Calculo de la probabilidad de verosimilitud marginal a este modelo y sus parámetros\n",
        "    self.log_marg_y = +1.*( 0.5*np.dot(alfa,self.Observacion) + np.sum(np.log(np.diag(cholL)),axis=0) + (float(len(self.Dominio))/2.)*np.log(2*np.pi))\n",
        "    if (self.log_marg_y > +90.):\n",
        "      self.log_marg_y = 90.\n",
        "    return self.log_marg_y\n",
        "\n",
        "  def der_log_prob_ver_hiper(self,*hiper):\n",
        "    '''\n",
        "    Derivada del logaritmo de la probabilidad de verosimilitud respecto hiperapametros,\n",
        "    devuelve un ndarray con cada componenete correspondiendo a la matriz del modelo respecto\n",
        "    los distintos. La expresion es la siguiente\n",
        "\n",
        "    1/2 traza[  {alfa*alfa.T - K⁻¹} dK/dh] ; alfa = K⁻¹\\y\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Actualizamos hiperparámetros\n",
        "    self.Hiper = hiper\n",
        "\n",
        "    # Definimos nueva covarianza\n",
        "    try:\n",
        "      covDom = self.calcCovM(self.xM,self.xM)\n",
        "    except:\n",
        "      raise NameError(\"Debes especificar primero el kernel que estas usando junto a sus hiperparametros con el metodo self.compCov\")\n",
        "\n",
        "\n",
        "    invCovDom = self.calcInvK(covDom)\n",
        "\n",
        "    cholL = np.linalg.cholesky(covDom)\n",
        "\n",
        "    # Calculo del vector alfa como solucion del sistema K_obs_obs*alfa = y_entrenamiento usando la descomposición cholesky anterior\n",
        "    alfa = scipy.linalg.cho_solve((cholL,True),self.Observacion)\n",
        "\n",
        "    alfa = np.reshape(alfa,(len(alfa),1))\n",
        "\n",
        "    self.Der_Hiper = hiper\n",
        "    derCovHiper = self.calcDerCovM(self.xM,self.xM)\n",
        "    return -0.5*np.trace(np.matmul(np.matmul(alfa,alfa.T)-invCovDom,derCovHiper),axis1=1,axis2=2) # La suma de la traza en un array de mas de 2 dimensiones debe especificarse sobre sus indices. En este caso es dimensióón 3.\n",
        "    \n",
        "\n",
        "  def prioriGP(self,*dominio):\n",
        "    '''\n",
        "    Computa los términos necesarios para extraer realizaciones del GP siguiendo la distribución a priori\n",
        "    '''\n",
        "\n",
        "    self.xtest = np.reshape(dominio_test,(len(dominio_test),1))\n",
        "\n",
        "    try:\n",
        "      self.Cov_priori = self.calcCovM(self.xtest,self.xtest,*self.Hiper)\n",
        "    except:\n",
        "      raise NameError(\"Debes especificar primero el kernel que estas usando junto a sus hiperparametros con el metodo self.compCov\")\n",
        "\n",
        "    # La siguiente matriz descompuesta es necesaria para generar procesos del GP.\n",
        "    self.L_priori = self.cholDescomp(self.Cov_pred)\n",
        "\n",
        "\n",
        "  def procesosPrioriGP(self):\n",
        "    '''\n",
        "    Genera realizaciones del proceso gaussiano dada la descomposicion cholesky de la matriz de covarianzas y la media correspondiente\n",
        "    '''\n",
        "    # Si no se ha calculado la descomposicióón de la covarianza se llama al método aqui\n",
        "    try:\n",
        "      return self.Mean_priori +  np.matmul(self.L_priori,(np.randn(len(self.xtest))).T)\n",
        "    except:\n",
        "      raise NameError(\"No se ha incluido los valores del dominio sobre los que muestrear el proceso o no se ha calculado la descomposición de la covarianza\")\n",
        "\n",
        "  def procesosCondicionadosGP(self):\n",
        "    '''\n",
        "    Genera realizaciones del proceso gaussiano dada la descomposicion cholesky de la matriz de covarianzas y la media correspondiente\n",
        "    '''\n",
        "    try:\n",
        "      return self.Media_pred + np.matmul(self.L_pred,(np.randn(len(self.xtest))).T)\n",
        "    except:\n",
        "      raise NameError(\"El GP no ha sido condicionado a las observaciones\")\n",
        "\n",
        "  def sigmaCalc(self,cov):\n",
        "    '''\n",
        "    Desviacion estándar punto a punto del proceso gausiano dado por la matriz de covarianza \n",
        "    Es la raiz cuadrado de los elementos de la diagonal de la matriz de covarianzas del proceso generado\n",
        "    '''\n",
        "    return np.sqrt(np.diag(cov))\n",
        "\n",
        "\n",
        "  def correlacionPuntoPunto(self):\n",
        "    '''\n",
        "    Obtiene la correlacion punto a punto dado un inputo concreto por el indice i_x\n",
        "    '''\n",
        "    try:\n",
        "      self.corrPP_pred = np.array([ self.K_pred[i,:] for i in range(len(self.K_pred)) ])\n",
        "    except:\n",
        "      print(\" Proceso gaussiano no entrenado aún, no existe K_pred \")\n",
        "    self.corrPP_prior = np.array([ self.K_prior[i,:] for i in range(len(self.K_prior)) ])\n",
        "\n",
        "  \n",
        "  def derivada_LO_CV(self,*args):\n",
        "    '''\n",
        "    Dadas unas observaciones particulares estima la derivada de la suma de los logaritmos de la evaluación de la distribución de probabilidad marginal de verosimilitud a que subconjuntos\n",
        "    de las observaciones están condicionadas al resto, respecto a los hiperparámetros del modelo. Si se encuentra el máximo de este valor, estamos asegurándonos de que con los hiperparámetros ajustados\n",
        "    las \"predicciones\" tras generar trayectorias del proceso gausiano que corresponderían a los subconjuntos excluidos sucesivamente son las más probables que el modelo puede hacer. Al ser la suma, esta optimización \n",
        "    es global y abarca la aproximación de observacion/test de todos los subconjuntos.\n",
        "    '''\n",
        "\n",
        "    if not isinstance(args[0], np.ndarray):\n",
        "      raise NameError(\"Las observaciones dadas para calcular la derivada del logaritmo de la distribución marginal de verosimilitud no es un array con datos\")\n",
        "    else:\n",
        "      observaciones = args[0]\n",
        "\n",
        "    # Vector auxiliar alpha y matriz auxiliar Z y la diagonal de la inversa de la distribucion a priori\n",
        "    try:\n",
        "      alpha = np.matmul(self.invK,observaciones)\n",
        "      invK_diag = np.diag(self.invK)\n",
        "      zeta = [np.matmul(self.invK,self.Derivadas[i]) for i in range(len(self.Derivadas)) ] # Es una lista de matrices\n",
        "      zeta_K = [np.diag(np.matmul(zeta[i],self.invK)) for i in range(len(self.Derivadas)) ]\n",
        "      alpha_invK = np.multiply(alpha,1./invK_diag)\n",
        "    except:\n",
        "      raise NameError(\"Necesitas calcular la matriz inversa de covarianzas de la distribución a priori\")\n",
        "    \n",
        "    termino_1 = [np.matmul(np.matmul(alpha_invK,zeta[i]),alpha) for i in range(len(self.Derivadas)) ] # Debido a Z esto es una lista\n",
        "    termino_2 = [np.sum(np.multiply(np.multiply(0.5*(1. + np.multiply(np.power(alpha,2),1./invK_diag)),zeta_K[i]),1./invK_diag)) for i in range(len(self.Derivadas)) ] # Debido a zeta_K esto es una lista\n",
        "\n",
        "    # Calculo. Es el valor de la derivada del logaritmo\n",
        "    self.derVerLOO = np.asarray([ termino_1[i] + termino_2[i] for i in range(len(self.Derivadas))])\n",
        "    "
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SciOihGB3HVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33dbefbd-0c75-4284-8870-2ef626504085"
      },
      "source": [
        "import time\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "#x_train = np.linspace(0.1,9.7,5)\n",
        "#y_train = 1.1*np.sin(2.*x_train) + np.asarray([random.gauss(0.,1.) for i in range(len(x_train))])\n",
        "x_train = np.array([-7.27980659640218,-6.3565864147485,-6.33349783214072,-5.85186882395103,-4.74692152231896,-4.08519569483122,-3.73024545572045,-2.78487668464404,-2.15092471829579,-0.902510956795564,0.496360463886639,0.756812013112905,1.00714394820637,2.37248404949064,2.46618962012525,4.24396641874347,4.33629137438754,4.89477951284853,5.81238911017895,6.14524950944106])\n",
        "y_train = np.array([-1.49888963305016,0.569441996545526,0.623268356304418,0.9063943036413,-0.376423208431739,-0.871911241143502,-0.803958546300539,1.07173322993408,2.29393880644365,2.62802707180373,0.412113574676584,-0.297490218195917,-0.660456836687934,-2.66828227995347,-2.09868165955797,-0.965605061863299,-0.758001691987733,-1.32241178751454,-0.725175367478586,-0.449178680954562])\n",
        "\n",
        "grp = 4\n",
        "#sigmas = np.linspace(0.01,3.,grp)\n",
        "#ls = np.linspace(0.01,10.,grp)\n",
        "\n",
        "sigmas = np.linspace(np.log(0.01),np.log(3.),grp)\n",
        "ls = np.linspace(np.log(0.01),np.log(10.),grp)\n",
        "\n",
        "xx, yy = np.meshgrid(ls,sigmas)\n",
        "\n",
        "sigmas = np.exp(sigmas)\n",
        "ls = np.exp(ls)\n",
        "\n",
        "\n",
        "probabilidades = np.zeros((grp,grp))\n",
        "\n",
        "s = 1.\n",
        "\n",
        "gp_prueba = gaussProcess(x_train,y_train,[0.,0.])\n",
        "\n",
        "gp_prueba.compCov(lambda x,y,lh,sigmah : gp_prueba.Kernels.exponencialCuadrada(x,y,1.,lh)+gp_prueba.Kernels.ruidoBlanco(x,y,sigmah)) # Aqui especificamos los hiperparametros\n",
        "funciones = np.asarray([lambda x,y,lh,sigmah: gp_prueba.Kernels.expCua_der_l(x,y,1.,lh), lambda x,y,lh,sigmah: gp_prueba.Kernels.rB_der_sigma(x,y,sigmah)])\n",
        "\n",
        "#gp_prueba.compCov(lambda x,y,sh,lh,sigmah : gp_prueba.Kernels.exponencialCuadrada(x,y,sh,lh)+gp_prueba.Kernels.ruidoBlanco(x,y,sigmah)) # Aqui especificamos los hiperparametros\n",
        "#funciones = np.asarray([lambda x,y,sh,lh,sigmah: gp_prueba.Kernels.expCua_der_s(x,y,sh,lh) , lambda x,y,sh,lh,sigmah: gp_prueba.Kernels.expCua_der_l(x,y,sh,lh), lambda x,y,sh,lh,sigmah: gp_prueba.Kernels.rB_der_sigma(x,y,sigmah)])\n",
        "gp_prueba.compDerCov(funciones)\n",
        "\n",
        "\n",
        "i = 0\n",
        "for sigma in sigmas:\n",
        "\n",
        "  j = 0\n",
        "  for l in ls:\n",
        "    #gp_prueba.Hiper = [s,l,sigma]\n",
        "    gp_prueba.Hiper = [l,sigma]\n",
        "    gp_prueba.log_prob_verosimilitud_datos(*gp_prueba.Hiper)\n",
        "    probabilidades[i,j] = gp_prueba.log_marg_y\n",
        "    resultado = gp_prueba.der_log_prob_ver_hiper(*gp_prueba.Hiper)\n",
        "    #print(\"logaritmo de la probabilidad de verosimilitud e hiperparametros:\",gp_prueba.log_marg_y)\n",
        "  \n",
        "    j = j + 1\n",
        "  \n",
        "  i = i + 1\n",
        "\n",
        "\n",
        "fig_3, ax_3 = plt.subplots()\n",
        "\n",
        "plotRange_sig = sigmas\n",
        "plotRange_l = ls\n",
        "\n",
        "heatmap = ax_3.pcolormesh(xx,yy,probabilidades,cmap='RdBu_r',vmin = probabilidades.min(), vmax = probabilidades.max())\n",
        "#ax_3.axis([plotRange_l.min(), plotRange_l.max(), plotRange_sig.min(), plotRange_sig.max()])\n",
        "fig_3.colorbar(heatmap, ax=ax_3)\n",
        "\n",
        "#ax_3.set_xscale('log')\n",
        "#ax_3.set_yscale('log')\n",
        "\n",
        "fig_3.show()\n",
        "\n",
        "#im = ax_3.imshow(probabilidades)\n",
        "\n",
        "#fig_3.colorbar(im, ax=ax_3)\n",
        "\n",
        "#fig_3.show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "print(\"partida :\",[1.,0.3],gp_prueba.log_prob_verosimilitud_datos(*[1.1,0.15]))\n",
        "#ppp = pruebaHMC()\n",
        "#afdaf = [np.random.randn(),np.random.randn(),np.random.randn(),np.random.randn(),np.random.randn()]\n",
        "#afdaf = [10485.50142632,   9058.50096209 , -8420.49975872, -10909.50244681 ,-311.49708436]\n",
        "#print(\"partida :\",afdaf)\n",
        "\n",
        "busqueda = hmc_annealing([1.,0.3],400.,.4,1,gp_prueba)\n",
        "#busqueda = hmc_annealing(afdaf,400.,0.4,10,ppp)\n",
        "busqueda.annealing()\n",
        "print(\"Finalmente el valor minimo calculado :\",gp_prueba.log_prob_verosimilitud_datos(busqueda.ObjetoHMC.Xo), \" teorico :\", [1.1,0.15])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- 0.3689846992492676 seconds ---\n",
            "partida : [1.0, 0.3] 21.388132887471357\n",
            "Paso de enfriamiento dado, temp actual : 400.0  aceptado promedio : 0.9868553068904088  dt : 0.5219092735316979  n : 1  total pasos probados : 102\n",
            "Paso de enfriamiento dado, temp actual : 296.32728827268716  aceptado promedio : 0.9774360610135748  dt : 0.6809732244959615  n : 1  total pasos probados : 101\n",
            "Paso de enfriamiento dado, temp actual : 219.52465443761056  aceptado promedio : 0.9981112663962317  dt : 0.8885156022280627  n : 1  total pasos probados : 100\n",
            "Paso de enfriamiento dado, temp actual : 162.62786389623966  aceptado promedio : 0.9985009414924504  dt : 1.1593113312010679  n : 1  total pasos probados : 100\n",
            "Paso de enfriamiento dado, temp actual : 120.47768476488085  aceptado promedio : 0.9980151685689997  dt : 1.5126383366605376  n : 1  total pasos probados : 100\n",
            "Paso de enfriamiento dado, temp actual : 89.25206405937193  aceptado promedio : 0.9971667370569761  dt : 1.9736499385067423  n : 1  total pasos probados : 100\n",
            "Paso de enfriamiento dado, temp actual : 66.11955528863463  aceptado promedio : 0.9970367868153707  dt : 2.5751655140298344  n : 1  total pasos probados : 100\n",
            "Paso de enfriamiento dado, temp actual : 48.98257130119276  aceptado promedio : 0.9975651365415814  dt : 3.360006906627981  n : 1  total pasos probados : 100\n",
            "Paso de enfriamiento dado, temp actual : 36.287181315765004  aceptado promedio : 0.9959386575006864  dt : 4.384046909249242  n : 1  total pasos probados : 100\n",
            "Paso de enfriamiento dado, temp actual : 26.882205095899913  aceptado promedio : 0.9952824669720889  dt : 5.720186843837892  n : 1  total pasos probados : 100\n",
            "Paso de enfriamiento dado, temp actual : 19.91482734714558  aceptado promedio : 0.992690663297527  dt : 7.4635464003325245  n : 1  total pasos probados : 102\n",
            "Paso de enfriamiento dado, temp actual : 14.753266960496006  aceptado promedio : 0.9869486843524807  dt : 9.738235199419167  n : 1  total pasos probados : 102\n",
            "Paso de enfriamiento dado, temp actual : 10.929488978917028  aceptado promedio : 0.9683607411359876  dt : 12.706188146024164  n : 1  total pasos probados : 102\n",
            "Paso de enfriamiento dado, temp actual : 8.096764578321757  aceptado promedio : 0.8269129133799805  dt : 17.076054368469997  n : 1  total pasos probados : 116\n",
            "Paso de enfriamiento dado, temp actual : 5.998230728191081  aceptado promedio : 0.8230038559308461  dt : 23.637252835168297  n : 1  total pasos probados : 129\n",
            "Paso de enfriamiento dado, temp actual : 4.4435986152969225  aceptado promedio : 0.6211850062087237  dt : 35.75346576227791  n : 1  total pasos probados : 164\n",
            "Paso de enfriamiento dado, temp actual : 3.291898819608012  aceptado promedio : 0.6805769668390188  dt : 52.50516984464136  n : 1  total pasos probados : 149\n",
            "Paso de enfriamiento dado, temp actual : 2.4386986262062553  aceptado promedio : 0.6244330287505934  dt : 79.41878040877695  n : 1  total pasos probados : 159\n",
            "Paso de enfriamiento dado, temp actual : 1.806632377045068  aceptado promedio : 0.6063713476452172  dt : 116.62915651149555  n : 1  total pasos probados : 175\n",
            "Paso de enfriamiento dado, temp actual : 1.3383861829885089  aceptado promedio : 0.6069905630670109  dt : 181.70442567054096  n : 1  total pasos probados : 168\n",
            "Paso de enfriamiento dado, temp actual : 0.9915008706665434  aceptado promedio : 0.5343047920698846  dt : 165.83672329000962  n : 1  total pasos probados : 181\n",
            "Paso de enfriamiento dado, temp actual : 0.7345219108115628  aceptado promedio : 0.5202681166694008  dt : 250.84292365210885  n : 1  total pasos probados : 186\n",
            "Paso de enfriamiento dado, temp actual : 0.5441472150191575  aceptado promedio : 0.40381201261480926  dt : 132.31475395986607  n : 1  total pasos probados : 247\n",
            "Paso de enfriamiento dado, temp actual : 0.40311417161940455  aceptado promedio : 0.4511371139918302  dt : 136.16154080174127  n : 1  total pasos probados : 229\n",
            "Paso de enfriamiento dado, temp actual : 0.2986343233506719  aceptado promedio : 0.4733043025358358  dt : 103.51423231174454  n : 1  total pasos probados : 207\n",
            "Paso de enfriamiento dado, temp actual : 0.22123374805913346  aceptado promedio : 0.4864444366317662  dt : 88.8911934303094  n : 1  total pasos probados : 207\n",
            "Paso de enfriamiento dado, temp actual : 0.16389399159191473  aceptado promedio : 0.5135778698232525  dt : 106.14073365666451  n : 1  total pasos probados : 179\n",
            "Paso de enfriamiento dado, temp actual : 0.12141565523154671  aceptado promedio : 0.49643113663185323  dt : 99.86781629755563  n : 1  total pasos probados : 195\n",
            "Paso de enfriamiento dado, temp actual : 0.08994692967153928  aceptado promedio : 0.5156921420372125  dt : 93.9656283543701  n : 1  total pasos probados : 184\n",
            "Paso de enfriamiento dado, temp actual : 0.06663432439505342  aceptado promedio : 0.4746516479705791  dt : 96.7845972050012  n : 1  total pasos probados : 221\n",
            "Paso de enfriamiento dado, temp actual : 0.04936392163467183  aceptado promedio : 0.467562481798293  dt : 59.45029243208643  n : 1  total pasos probados : 219\n",
            "Paso de enfriamiento dado, temp actual : 0.036569692591269375  aceptado promedio : 0.47233396220431795  dt : 64.9629396984365  n : 1  total pasos probados : 227\n",
            "Paso de enfriamiento dado, temp actual : 0.02709149459634156  aceptado promedio : 0.4739949296385348  dt : 52.48894082288773  n : 1  total pasos probados : 209\n",
            "Paso de enfriamiento dado, temp actual : 0.020069872822470112  aceptado promedio : 0.46872137458959157  dt : 47.90523908764741  n : 1  total pasos probados : 202\n",
            "Paso de enfriamiento dado, temp actual : 0.014868127473650693  aceptado promedio : 0.43494323143096003  dt : 34.143491105807996  n : 1  total pasos probados : 233\n",
            "Paso de enfriamiento dado, temp actual : 0.011014579739898863  aceptado promedio : 0.5453237092638399  dt : 34.143491105807996  n : 1  total pasos probados : 189\n",
            "Paso de enfriamiento dado, temp actual : 0.008159801364468785  aceptado promedio : 0.4949593856116322  dt : 33.119186372633756  n : 1  total pasos probados : 199\n",
            "Paso de enfriamiento dado, temp actual : 0.006044929527942013  aceptado promedio : 0.4810421098260448  dt : 25.178234117845214  n : 1  total pasos probados : 218\n",
            "Paso de enfriamiento dado, temp actual : 0.004478193937036376  aceptado promedio : 0.4672599590790272  dt : 24.422887094309857  n : 1  total pasos probados : 195\n",
            "Paso de enfriamiento dado, temp actual : 0.0033175276643029484  aceptado promedio : 0.48451424794578485  dt : 18.567037311108102  n : 1  total pasos probados : 207\n",
            "Paso de enfriamiento dado, temp actual : 0.002457684941331284  aceptado promedio : 0.48246002718093467  dt : 16.945633643840964  n : 1  total pasos probados : 203\n",
            "Paso de enfriamiento dado, temp actual : 0.001820697785233296  aceptado promedio : 0.48092101284426947  dt : 12.882597004629638  n : 1  total pasos probados : 207\n",
            "Paso de enfriamiento dado, temp actual : 0.0013488060936556738  aceptado promedio : 0.5386139493289342  dt : 12.882597004629638  n : 1  total pasos probados : 180\n",
            "Paso de enfriamiento dado, temp actual : 0.000999220130346654  aceptado promedio : 0.49460132997939454  dt : 11.39460611887406  n : 1  total pasos probados : 214\n",
            "Paso de enfriamiento dado, temp actual : 0.0007402404790327633  aceptado promedio : 0.45253230044311604  dt : 9.784935983726575  n : 1  total pasos probados : 233\n",
            "Paso de enfriamiento dado, temp actual : 0.0005483836345536338  aceptado promedio : 0.46889830031068896  dt : 6.585526992583924  n : 1  total pasos probados : 215\n",
            "Paso de enfriamiento dado, temp actual : 0.0004062525884009968  aceptado promedio : 0.502030937091969  dt : 6.783092802361442  n : 1  total pasos probados : 187\n",
            "Paso de enfriamiento dado, temp actual : 0.0003009593196865685  aceptado promedio : 0.4840042490695676  dt : 6.776988018839316  n : 1  total pasos probados : 205\n",
            "Paso de enfriamiento dado, temp actual : 0.00022295614770778425  aceptado promedio : 0.5069830970289697  dt : 5.152076776051517  n : 1  total pasos probados : 197\n",
            "Paso de enfriamiento dado, temp actual : 0.00016516997663493086  aceptado promedio : 0.49109681663550736  dt : 4.29153572169577  n : 1  total pasos probados : 203\n",
            "Resultado final: [ 6211.56601946 -4995.51711292]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-212d20e60cc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m#busqueda = hmc_annealing(afdaf,400.,0.4,10,ppp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mbusqueda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannealing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finalmente el valor minimo calculado :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgp_prueba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob_verosimilitud_datos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbusqueda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjetoHMC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" teorico :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-a3923fd0c4df>\u001b[0m in \u001b[0;36mlog_prob_verosimilitud_datos\u001b[0;34m(self, *hiper)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;31m# Primero se realiza la descomposición cholesky de la adición de la covarianza de la distribución a priorir y el término de ruido.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m#cholL = np.linalg.cholesky(self.calcCovM(dominio,dominio,*self.Hiper))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mcholL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcCovM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Calculo del vector alfa como solucion del sistema K*alfa = y_entrenamiento usando la descomposición cholesky anterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-a3923fd0c4df>\u001b[0m in \u001b[0;36mcalcCovM\u001b[0;34m(self, xM, yM)\u001b[0m\n\u001b[1;32m     22\u001b[0m     '''\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Llamamos la funcion Ker que nos debe devolver como salida una matriz cuyos elementos calculados representan k(x,y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHiper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcalcDerCovM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2091\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m             \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m             \u001b[0;31m# Convert args to object arrays first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_get_ufunc_and_otypes\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2120\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2121\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2123\u001b[0m             \u001b[0;31m# Performance note: profiling indicates that -- for simple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: <lambda>() missing 1 required positional argument: 'sigmah'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAwhLz3iOwtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class hmc_annealing():\n",
        "\n",
        "  def __init__(self,x,temp,dt,n,gp):\n",
        "    self.Temp = temp\n",
        "    self.Dt = dt\n",
        "    self.N = n\n",
        "    self.GP = gp\n",
        "    self.ObjetoHMC = self.hybridMontecarlo(x,self.Temp,self.Dt,self.N,self.GP)\n",
        "\n",
        "  def pasoAnnealing(self):\n",
        "    #print(\"Fuerzas involucradas iniciales :\",self.GP.der_log_prob_ver_hiper(*self.ObjetoHMC.Xo))\n",
        "    self.ObjetoHMC.Po = np.sqrt(self.Temp)*np.random.randn(len(self.ObjetoHMC.Xo)) # Siempre muestreamos la distribucion de momentos antes de cada trayectoria\n",
        "    #self.ObjetoHMC.Po = self.Temp*np.random.randn(len(self.ObjetoHMC.Xo)) # Siempre muestreamos la distribucion de momentos antes de cada trayectoria\n",
        "\n",
        "    self.Ho = self.GP.log_prob_verosimilitud_datos(*self.ObjetoHMC.Xo) + 0.5*np.sum(np.power(self.ObjetoHMC.Po,2),axis=0)\n",
        "    self.ObjetoHMC.ejecutarTrayectoria(self.Dt)\n",
        "    #if (self.ObjetoHMC.Xo < 0.).any():\n",
        "    #  # Hiperparametros deben ser positivos\n",
        "    #  self.acc_prob = 0.\n",
        "    #else:\n",
        "    self.H = self.GP.log_prob_verosimilitud_datos(*self.ObjetoHMC.Xo) + 0.5*np.sum(np.power(self.ObjetoHMC.Po,2),axis=0)\n",
        "    #print(\"tras trayectoria :\",self.ObjetoHMC.Xo)\n",
        "    self.Dif_H = self.H-self.Ho\n",
        "    #print(\"Dif_H :\",self.Dif_H)\n",
        "    self.acc_prob = np.minimum(1.,np.exp(-(1./self.Temp)*self.Dif_H))\n",
        "\n",
        "  def aceptacion(self):\n",
        "    unif = np.random.uniform()\n",
        "    if unif > self.acc_prob:\n",
        "      # No se acepta la nueva configuracon\n",
        "      self.ObjetoHMC.Xo = self.prev_Xo\n",
        "      self.Total = self.Total + 1\n",
        "      return 0\n",
        "    else:\n",
        "      # Se acepta\n",
        "      self.Total = self.Total + 1\n",
        "      self.acc_prob_a = self.acc_prob_a*((self.Total-1)/self.Total) + (self.acc_prob/self.Total)\n",
        "      return 1\n",
        "\n",
        "    #print(unif,self.acc_prob,self.aceptados)\n",
        "\n",
        "  def forzarAceptacion(self):\n",
        "    '''\n",
        "    Si variamos dt es un metodo de ajustar la probabilidad de aceptacion dinámicamente. Lo hacemos sondeando el promedio de la \n",
        "    probabilidad de aceptacion dinamicamente\n",
        "    '''\n",
        "    self.SubTotal = self.SubTotal + 1\n",
        "    if (self.SubTotal > 10):\n",
        "      if (self.acc_prob_a < 0.45):\n",
        "        # Debemos intentar mejorar la aceptacion\n",
        "        self.Dt = self.Dt - self.Dt*(0.03)\n",
        "      elif (self.acc_prob_a > 0.55):\n",
        "        # Debemos intentar reducir la aceptacióón\n",
        "        self.Dt = self.Dt + self.Dt*(0.03)\n",
        "      self.SubTotal = 0\n",
        "\n",
        "  def reducirT(self,To,astep):\n",
        "    self.Temp = To*np.exp(-0.3*float(astep))\n",
        "\n",
        "  def annealing(self):\n",
        "    an_step = 0\n",
        "    self.To = self.Temp\n",
        "    while an_step < 50:\n",
        "      self.aceptados = 0\n",
        "      self.Total = 0\n",
        "      self.acc_prob_a = 0.\n",
        "      self.SubTotal = 0.\n",
        "      while self.aceptados < 100:\n",
        "        self.prev_Xo = self.ObjetoHMC.Xo\n",
        "        self.pasoAnnealing()\n",
        "        self.aceptados = self.aceptados + self.aceptacion()\n",
        "        self.forzarAceptacion()\n",
        "      an_step = an_step + 1\n",
        "      print(\"Paso de enfriamiento dado, temp actual :\",self.Temp, \" aceptado promedio :\", self.acc_prob_a,\" dt :\", self.Dt, \" n :\",self.N, \" total pasos probados :\", self.Total)\n",
        "      self.reducirT(self.To,an_step)\n",
        "    print(\"Resultado final:\",self.ObjetoHMC.Xo)\n",
        "\n",
        "  def correlacionExploracion(self):\n",
        "    '''\n",
        "    Dada una temperatura se analiza la correlacion unitaria entre entre el valor de la energia muestreada en función de cuantas transiciones sobre el espacio de fase son aceptadas\n",
        "    '''\n",
        "\n",
        "    arrayEnergias = np.zeros(300,100) # 300 promedios de 100 aceptaciones de longitud\n",
        "    for i in range(300):\n",
        "      self.aceptados = 0\n",
        "      arrayEnergias[i,self.aceptados] = self.GP.log_prob_verosimilitud_datos(*self.ObjetoHMC.Xo)\n",
        "      while self.aceptados < 100:\n",
        "        self.prev_Xo = self.ObjetoHMC.Xo\n",
        "        self.pasoAnnealing()\n",
        "        if self.aceptacion() == 1:\n",
        "          self.aceptados += 1\n",
        "          arrayEnergias[i,self.aceptados] = self.GP.log_prob_verosimilitud_datos(*self.ObjetoHMC.Xo)\n",
        "\n",
        "    np.sum(arrayEnergias,arrayEnergias.T)\n",
        "      \n",
        "\n",
        "\n",
        "  class hybridMontecarlo():\n",
        "\n",
        "    def __init__(self,x,temp,dt,n,gp):\n",
        "      self.Xo = np.asarray(x) # En este formalismo de procesos gausianos son una lista de hiperparámetros\n",
        "      self.Po = None\n",
        "      self.Temp = temp\n",
        "      self.Dt = dt\n",
        "      self.t = 0. # Inicializacion del tiempo\n",
        "      self.N = n\n",
        "      self.Nd = len(self.Xo)\n",
        "      self.GP = gp \n",
        "      #self.trayectoria, self.ax_tray = plt.subplots(1,1)\n",
        "    def updateStepIter(self,):\n",
        "      '''\n",
        "      Ejecuta avance en el integrador simpléctico y actualiza las fuerzas calculadas\n",
        "      '''\n",
        "      self.update_x_verlet()\n",
        "      self.F = self.GP.der_log_prob_ver_hiper(*self.X)\n",
        "      self.update_p_verlet()\n",
        "      self.Fo = self.F\n",
        "      self.t += self.Dt\n",
        "\n",
        "    def update_x_verlet(self):\n",
        "      '''\n",
        "      La operacion se realiza sobre ndarray\n",
        "      '''\n",
        "      self.X = self.Xo + self.Dt*(self.Po + (0.5*self.Dt*self.Fo))\n",
        "      self.Xo = self.X\n",
        "\n",
        "    def update_p_verlet(self):\n",
        "      '''\n",
        "      La operacion se realiza sobre ndarray\n",
        "      '''\n",
        "      self.P = self.Po + 0.5*self.Dt*(self.Fo + self.F)\n",
        "      self.Po = self.P\n",
        "\n",
        "    #@numba.njit\n",
        "    def ejecutarTrayectoria(self,tray_dt):\n",
        "      '''\n",
        "      Desarrolla una trayectoria en el integrador simplectico de self.N iteraciones\n",
        "      '''\n",
        "      self.Dt = tray_dt\n",
        "      self.Fo = self.GP.der_log_prob_ver_hiper(*self.Xo)   \n",
        "      for index_t in range(self.N):\n",
        "        self.updateStepIter()\n",
        "        #self.ploteado(False)\n",
        "      #self.ploteado(True)\n",
        "\n",
        "\n",
        "    def ploteado(self,mostrar):\n",
        "      '''\n",
        "      Muestra punto x integrado hasta ahora frente al indice del tiempo\n",
        "      '''\n",
        "      for i in range(self.Nd):\n",
        "        self.ax_tray.plot(self.t,self.X[i],'ob')\n",
        "      if mostrar == True:\n",
        "        print(\"se ha mostrado\")\n",
        "        self.trayectoria.show()"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VpUiW0BdOrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class pruebaHMC():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def log_prob_verosimilitud_datos(self,*hiper):\n",
        "    arrayEntrada = np.asarray(hiper)\n",
        "    return (1./10.)*np.sum(np.sin(8*np.pi*arrayEntrada)/np.sin(2*np.pi*arrayEntrada))\n",
        "\n",
        "  def der_log_prob_ver_hiper(self,*hiper):\n",
        "    return np.asarray([-(np.pi/5.)*(   (4*np.cos(8*np.pi*i)*np.sin(2*np.pi*i)-np.sin(8*np.pi*i)*np.cos(2*np.pi*i)     ) /  np.power(np.sin(2*np.pi*i),2)   ) for i in hiper])\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Ogka96GyNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class datosVisualizacion():\n",
        "  '''\n",
        "  Esta clase contiene los datos de entrenamiento y realizaciones de los procesos gaussianos para la visualización de los mismos\n",
        "  '''\n",
        "  def __init__(self,Titulo):\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "    self.sigma = []\n",
        "    self.realizacion_dict = {}\n",
        "    self.titulo = Titulo\n",
        "    self.inicializacionPlot()\n",
        "\n",
        "  def inicializacionPlot(self):\n",
        "    '''\n",
        "    Inicializa el objeto matplotlib\n",
        "    '''\n",
        "    self.fig,self.ax = plt.subplots(nrows=1,ncols=1,figsize=(10,10))\n",
        "    self.ax.set_title(self.titulo)\n",
        "    self.fig_hm, self.ax_hm = plt.subplots(nrows=1,ncols=1,figsize=(10,10))\n",
        "\n",
        "  def add_covariance(self, cov):\n",
        "    '''\n",
        "    Añada matriz de covarianza y alternativamente calcula la covarianza del proceso gaussiano sobre el dominio\n",
        "    '''\n",
        "\n",
        "    self.cov_GP = cov\n",
        "\n",
        "  def add_mean(self,X,Mean):\n",
        "    '''\n",
        "    Almacena la desviación estándar de un proceso gausiano dado\n",
        "    '''\n",
        "    self.x_mean = X\n",
        "    self.mean = Mean\n",
        "\n",
        "\n",
        "  def add_sigma(self,X,Sigma):\n",
        "    '''\n",
        "    Almacena la desviación estándar de un proceso gausiano dado\n",
        "    '''\n",
        "    self.x_sigma = X\n",
        "    self.sigma = Sigma\n",
        "    self.meanMarker = 'r-'\n",
        "\n",
        "  def add_train_data(self,X,Y):\n",
        "    self.xtrain = X\n",
        "    self.ytrain = Y\n",
        "    self.marker = 'b*'\n",
        "\n",
        "  def add_data(self,X,Y,label):\n",
        "    self.x.append(X)\n",
        "    self.y.append(Y)\n",
        "    self.realizacion_dict[label] = len(self.x)-1\n",
        "\n",
        "  def add_corrPuntoPunto(self,corrPP, corrInputs):\n",
        "    '''\n",
        "    Añade correlaciones punto a punto sobre un dominio dado por los valores corrInputs\n",
        "    '''\n",
        "    self.corrPP = corrPP\n",
        "    self.corr_in = corrInputs\n",
        "\n",
        "  def add_data_plot(self,label):\n",
        "    try:\n",
        "      self.ax.plot(self.x[self.realizacion_dict[label]],self.y[self.realizacion_dict[label]],'g--')\n",
        "    except:\n",
        "      print('No existe datos referentes a etiqueta :',label)\n",
        "\n",
        "  def add_train_data_plot(self):\n",
        "    try:\n",
        "      self.ax.plot(self.xtrain,self.ytrain,self.marker)\n",
        "    except:\n",
        "      print('No existen datos de entrenamiento')\n",
        "\n",
        "  def add_mean_plot(self):\n",
        "    try:\n",
        "      self.ax.plot(self.x_mean,self.mean,self.meanMarker)\n",
        "    except:\n",
        "      print('No existen datos de entrenamiento')\n",
        "\n",
        "\n",
        "\n",
        "  def contorno_sigma(self):\n",
        "    '''\n",
        "    Muestra en pantalla contorno de 1 y 2 veces los valores de sigma del proceso gausiano\n",
        "    '''\n",
        "    self.ax.fill(np.concatenate((self.x_sigma,self.x_sigma[::-1])),\n",
        "             np.concatenate((self.mean - 1.9600 * self.sigma,\n",
        "                             (self.mean + 1.9600 * self.sigma)[::-1])),\n",
        "             alpha=.45, fc='y', ec='None', label='95% confidence interval')\n",
        "    self.ax.fill(np.concatenate((self.x_sigma,self.x_sigma[::-1])),\n",
        "             np.concatenate((self.mean - 1.000 * self.sigma,\n",
        "                             (self.mean + 1.000 * self.sigma)[::-1])),\n",
        "             alpha=.35, fc='b', ec='None', label='68% confidence interval')\n",
        "    \n",
        "  def plotCorrPuntoPunto(self,indices):\n",
        "    '''\n",
        "    Visualiza correlaciones punto a punto para los inputs dados por sus indices en la lista indices como argumento del metodo\n",
        "    '''\n",
        "\n",
        "    ' Generamos y visualizamos las imagen directamente en este metodo'\n",
        "    self.fig_cpp, self.ax_cpp = plt.subplots(nrows=1,ncols=1,figsize=(10,10))\n",
        "    try:\n",
        "      [self.ax_cpp.plot(self.corr_in,self.corrPP[indice],'-') for indice in indices] \n",
        "    except:\n",
        "      print('No existe datos referentes referentes a correlaciones punto a punto')\n",
        "    self.ax_cpp.grid(True)\n",
        "    self.fig_cpp.show()\n",
        "\n",
        "\n",
        "\n",
        "  def mostrarPlot(self):\n",
        "    '''\n",
        "    Muestra plot\n",
        "    '''\n",
        "    self.add_mean_plot()\n",
        "    self.add_train_data_plot()\n",
        "    self.contorno_sigma()\n",
        "    self.ax.grid(True)\n",
        "    self.fig.show()\n",
        "\n",
        "  def hmapCov(self):\n",
        "    '''\n",
        "    Visualiza covarianza como campo bidimensional\n",
        "    '''\n",
        "    plotRange = np.arange(len(self.cov_GP))\n",
        "    heatmap = self.ax_hm.pcolormesh(self.cov_GP,cmap='RdBu_r')\n",
        "    self.ax_hm.axis([plotRange.min(), plotRange.max(), plotRange.max(), plotRange.min()])\n",
        "    self.fig_hm.colorbar(heatmap, ax=self.ax_hm)\n",
        "    self.fig_hm.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKwMJ0tWfspi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}